{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9tNzYmvAhtr"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/eeg2025/startkit/blob/main/challenge_1.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Model Saving/Loading Ready!\n",
      "Use save_model_as_pt() to save as .pt file\n",
      "Use load_model_from_pt() to load from .pt file\n"
     ]
    }
   ],
   "source": [
    "# Save Model as .pt File\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_as_pt(model, save_dir=\"models\", filename=None):\n",
    "    \"\"\"\n",
    "    Save EEGConformer model as .pt file (PyTorch format)\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"eegconformer_challenge1_{timestamp}.pt\"\n",
    "    \n",
    "    model_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "    # Save only the model state dict (standard .pt format)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    print(f\"âœ… Model saved as .pt file: {model_path}\")\n",
    "    print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"   File size: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "def load_model_from_pt(model_path, device):\n",
    "    \"\"\"\n",
    "    Load EEGConformer model from .pt file\n",
    "    \"\"\"\n",
    "    # Model configuration (must match training configuration)\n",
    "    model_config = {\n",
    "        'n_chans': 129,\n",
    "        'n_outputs': 1,\n",
    "        'n_times': 200,\n",
    "        'sfreq': 100,\n",
    "        'n_filters_time': 40,\n",
    "        'filter_time_length': 25,\n",
    "        'pool_time_length': 75,\n",
    "        'pool_time_stride': 15,\n",
    "        'att_depth': 6,\n",
    "        'att_heads': 10,\n",
    "        'drop_prob': 0.5,\n",
    "        'att_drop_prob': 0.5,\n",
    "        'final_fc_length': 'auto',\n",
    "        'return_features': False,\n",
    "        'activation': torch.nn.ELU,\n",
    "        'activation_transfor': torch.nn.GELU,\n",
    "    }\n",
    "    \n",
    "    # Create model with same configuration\n",
    "    model = EEGConformer(**model_config)\n",
    "    \n",
    "    # Load state dict\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"âœ… Model loaded from .pt file: {model_path}\")\n",
    "    print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# Save model after training:\n",
    "# model_path = save_model_as_pt(model_conformer, filename=\"my_eegconformer.pt\")\n",
    "\n",
    "# Load model for inference:\n",
    "# loaded_model = load_model_from_pt(model_path, device)\n",
    "\n",
    "print(\"ðŸ’¾ Model Saving/Loading Ready!\")\n",
    "print(\"Use save_model_as_pt() to save as .pt file\")\n",
    "print(\"Use load_model_from_pt() to load from .pt file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Example Usage:\n",
      "1. Train your model\n",
      "2. Save with: save_model_as_pt(model_conformer, filename='my_model.pt')\n",
      "3. Load with: loaded_model = load_model_from_pt('my_model.pt', device)\n",
      "4. Use loaded_model for inference\n",
      "\n",
      "ðŸ“ Found existing .pt files:\n",
      "   - weights_challenge_2_10_30.pt (1.60 MB)\n",
      "   - Labram_10_29.pt (22.37 MB)\n",
      "   - model_weights_challenge_2.pt (0.25 MB)\n",
      "   - model_weights_challenge_2_r1_r5.pt (0.25 MB)\n",
      "   - eegminer_10_28.pt (0.19 MB)\n",
      "   - weights_challenge_2_10_29.pt (22.37 MB)\n",
      "   - ATCNet_1_10_28.pt (0.10 MB)\n",
      "   - Deep4Net_1_10_28.pt (0.10 MB)\n",
      "   - weights_eegnex_10_28.pt (0.25 MB)\n",
      "   - submission_1/model_weights_challenge_1.pt (0.25 MB)\n"
     ]
    }
   ],
   "source": [
    "# Example: Save and Load Model\n",
    "# This cell shows how to save your trained model as a .pt file\n",
    "\n",
    "# After training your model, save it:\n",
    "# model_path = save_model_as_pt(\n",
    "#     model=model_conformer,\n",
    "#     save_dir=\"models\",\n",
    "#     filename=\"eegconformer_final.pt\"\n",
    "# )\n",
    "\n",
    "# To load the model later for inference:\n",
    "# loaded_model = load_model_from_pt(\"models/eegconformer_final.pt\", device)\n",
    "\n",
    "# For submission purposes, you might want to save with a specific name:\n",
    "# submission_path = save_model_as_pt(\n",
    "#     model=model_conformer,\n",
    "#     save_dir=\"submission_1\",\n",
    "#     filename=\"model_weights_challenge_1.pt\"\n",
    "# )\n",
    "\n",
    "print(\"ðŸ“ Example Usage:\")\n",
    "print(\"1. Train your model\")\n",
    "print(\"2. Save with: save_model_as_pt(model_conformer, filename='my_model.pt')\")\n",
    "print(\"3. Load with: loaded_model = load_model_from_pt('my_model.pt', device)\")\n",
    "print(\"4. Use loaded_model for inference\")\n",
    "\n",
    "# Check if you have any existing .pt files\n",
    "import glob\n",
    "existing_models = glob.glob(\"*.pt\") + glob.glob(\"models/*.pt\") + glob.glob(\"submission_1/*.pt\")\n",
    "if existing_models:\n",
    "    print(f\"\\nðŸ“ Found existing .pt files:\")\n",
    "    for model_file in existing_models:\n",
    "        size_mb = os.path.getsize(model_file) / 1024 / 1024\n",
    "        print(f\"   - {model_file} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"\\nðŸ“ No existing .pt files found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZK2rYtCSgEl",
    "outputId": "3d62062a-978d-4839-96b9-a6e5e4c71bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Training will be carried out on CPU, which might be slower.\n",
      "\n",
      "If running on Google Colab, you can request a GPU runtime by clicking\n",
      "`Runtime/Change runtime type` in the top bar menu, then selecting 'T4 GPU'\n",
      "under 'Hardware accelerator'.\n"
     ]
    }
   ],
   "source": [
    "# Identify whether a CUDA-enabled GPU is available\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    msg ='CUDA-enabled GPU found. Training should be faster.'\n",
    "else:\n",
    "    msg = (\n",
    "        \"No GPU found. Training will be carried out on CPU, which might be \"\n",
    "        \"slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by\"\n",
    "        \" clicking\\n`Runtime/Change runtime type` in the top bar menu, then \"\n",
    "        \"selecting \\'T4 GPU\\'\\nunder \\'Hardware accelerator\\'.\"\n",
    "    )\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "# Define a method for training one epoch\n",
    "def train_one_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: Module,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    scheduler: Optional[LRScheduler],\n",
    "    epoch: int,\n",
    "    device,\n",
    "    print_batch_stats: bool = True,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    sum_sq_err = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(dataloader), total=len(dataloader), disable=not print_batch_stats\n",
    "    )\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        X, y = batch[0], batch[1]\n",
    "        X, y = X.to(device).float(), y.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Flatten to 1D for regression metrics and accumulate squared error\n",
    "        preds_flat = preds.detach().view(-1)\n",
    "        y_flat = y.detach().view(-1)\n",
    "        sum_sq_err += torch.sum((preds_flat - y_flat) ** 2).item()\n",
    "        n_samples += y_flat.numel()\n",
    "\n",
    "        if print_batch_stats:\n",
    "            running_rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch}, Batch {batch_idx + 1}/{len(dataloader)}, \"\n",
    "                f\"Loss: {loss.item():.6f}, RMSE: {running_rmse:.6f}\"\n",
    "            )\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
    "    return avg_loss, rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCGGky2hAhtz"
   },
   "source": [
    "For the challenge, we will need two significant dependencies: `braindecode` and `eegdash`. The libraries will install PyTorch, Pytorch Audio, Scikit-learn, MNE, MNE-BIDS, and many other packages necessary for the many functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available releases: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "ðŸ“ Loading data from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 745 recordings from Release R5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "available_releases = []\n",
    "\n",
    "if data_dir.exists():\n",
    "    for item in data_dir.iterdir():\n",
    "        if item.is_dir() and item.name.startswith(\"release_\"):\n",
    "            release_num = item.name.split(\"_\")[1]\n",
    "            available_releases.append(int(release_num))\n",
    "\n",
    "available_releases.sort()\n",
    "print(f\"Available releases: {available_releases}\")\n",
    "\n",
    "# Use release 5 (like original) or choose a different one\n",
    "RELEASE_ID = 5  # Change this to use a different release\n",
    "RELEASE_DIR = Path(f\"data/release_{RELEASE_ID}\")\n",
    "\n",
    "if not RELEASE_DIR.exists():\n",
    "    # print(f\"âŒ Release {RELEASE_ID} folder not found: {RELEASE_DIR}\")\n",
    "    print(f\"Available releases: {available_releases}\")\n",
    "    # Use the first available release if R5 doesn't exist\n",
    "    if available_releases:\n",
    "        RELEASE_ID = available_releases[0]\n",
    "        RELEASE_DIR = Path(f\"data/release_{RELEASE_ID}\")\n",
    "        print(f\"ðŸ”„ Using Release {RELEASE_ID} instead\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No release folders found in data/\")\n",
    "\n",
    "print(f\"ðŸ“ Loading data from: {RELEASE_DIR.resolve()}\")\n",
    "\n",
    "from eegdash.dataset import EEGChallengeDataset\n",
    "\n",
    "# Load from the specific release folder\n",
    "dataset_ccd = EEGChallengeDataset(task=\"contrastChangeDetection\",\n",
    "                                  release=f\"R{RELEASE_ID}\", cache_dir=RELEASE_DIR,\n",
    "                                  mini=False)\n",
    "\n",
    "print(f\"Loaded dataset with {len(dataset_ccd.datasets)} recordings from Release R{RELEASE_ID}\")\n",
    "\n",
    "# Helper function to load different releases for transfer learning\n",
    "def load_release_data(release_id, task=\"contrastChangeDetection\", mini=False):\n",
    "    \"\"\"\n",
    "    Load data from a specific release folder\n",
    "    \n",
    "    Args:\n",
    "        release_id (int): Release number (1-11)\n",
    "        task (str): Task name\n",
    "        mini (bool): Whether to use mini dataset\n",
    "    \n",
    "    Returns:\n",
    "        EEGChallengeDataset: Loaded dataset\n",
    "    \"\"\"\n",
    "    release_dir = Path(f\"data/release_{release_id}\")\n",
    "    \n",
    "    if not release_dir.exists():\n",
    "        raise FileNotFoundError(f\"Release {release_id} folder not found: {release_dir}\")\n",
    "    \n",
    "    print(f\"Loading Release R{release_id} from: {release_dir.resolve()}\")\n",
    "    \n",
    "    dataset = EEGChallengeDataset(\n",
    "        task=task,\n",
    "        release=f\"R{release_id}\",\n",
    "        cache_dir=release_dir,\n",
    "        mini=mini\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {len(dataset.datasets)} recordings from Release R{release_id}\")\n",
    "    return dataset\n",
    "\n",
    "# Example usage for transfer learning:\n",
    "# source_data = load_release_data(1)  # Load release 1 for pre-training\n",
    "# target_data = load_release_data(5)  # Load release 5 for target task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Setting up Transfer Learning with Multiple Releases\n",
      "============================================================\n",
      "ðŸ“š Loading source releases for pre-training: [1, 2, 3, 4, 6, 7, 8, 9, 10, 11]\n",
      "Loading Release R1 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 293 recordings from Release R1\n",
      "âœ… Added Release R1: 293 recordings\n",
      "Loading Release R2 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 301 recordings from Release R2\n",
      "âœ… Added Release R2: 301 recordings\n",
      "Loading Release R3 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 388 recordings from Release R3\n",
      "âœ… Added Release R3: 388 recordings\n",
      "Loading Release R4 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 756 recordings from Release R4\n",
      "âœ… Added Release R4: 756 recordings\n",
      "Loading Release R6 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 232 recordings from Release R6\n",
      "âœ… Added Release R6: 232 recordings\n",
      "Loading Release R7 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 529 recordings from Release R7\n",
      "âœ… Added Release R7: 529 recordings\n",
      "Loading Release R8 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 512 recordings from Release R8\n",
      "âœ… Added Release R8: 512 recordings\n",
      "Loading Release R9 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 655 recordings from Release R9\n",
      "âœ… Added Release R9: 655 recordings\n",
      "Loading Release R10 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_10\n",
      "Loaded 212 recordings from Release R10\n",
      "âœ… Added Release R10: 212 recordings\n",
      "Loading Release R11 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n",
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 767 recordings from Release R11\n",
      "âœ… Added Release R11: 767 recordings\n",
      "\n",
      "ðŸŽ¯ Loading target release: R5\n",
      "Loading Release R5 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 745 recordings from Release R5\n",
      "âœ… Target dataset ready: 745 recordings\n",
      "\n",
      "ðŸ“Š Transfer Learning Setup Summary:\n",
      "Source releases: 10 releases, 4645 recordings\n",
      "Target release: R5, 745 recordings\n",
      "You can now use source data for pre-training and target data for fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# TRANSFER LEARNING SETUP: Load multiple releases for pre-training and target tasks\n",
    "print(\"ðŸ”„ Setting up Transfer Learning with Multiple Releases\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Option 1: Load source data for pre-training (e.g., releases 1-3)\n",
    "source_releases = [1, 2, 3, 4, 6, 7, 8, 9, 10, 11]  # Releases to use for pre-training\n",
    "source_datasets = []\n",
    "\n",
    "print(f\"ðŸ“š Loading source releases for pre-training: {source_releases}\")\n",
    "for release_id in source_releases:\n",
    "    try:\n",
    "        dataset = load_release_data(release_id, mini=False)\n",
    "        source_datasets.append(dataset)\n",
    "        print(f\"âœ… Added Release R{release_id}: {len(dataset.datasets)} recordings\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load Release R{release_id}: {e}\")\n",
    "\n",
    "# Option 2: Load target data (e.g., release 5)\n",
    "target_release = 5\n",
    "print(f\"\\nðŸŽ¯ Loading target release: R{target_release}\")\n",
    "try:\n",
    "    target_dataset = load_release_data(target_release, mini=False)\n",
    "    print(f\"âœ… Target dataset ready: {len(target_dataset.datasets)} recordings\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load target release: {e}\")\n",
    "    # Use the main dataset as fallback\n",
    "    target_dataset = dataset_ccd\n",
    "    print(f\"ðŸ”„ Using main dataset as target: {len(target_dataset.datasets)} recordings\")\n",
    "\n",
    "# Summary\n",
    "total_source_recordings = sum(len(dataset.datasets) for dataset in source_datasets)\n",
    "print(f\"\\nðŸ“Š Transfer Learning Setup Summary:\")\n",
    "print(f\"Source releases: {len(source_datasets)} releases, {total_source_recordings} recordings\")\n",
    "print(f\"Target release: R{target_release}, {len(target_dataset.datasets)} recordings\")\n",
    "print(f\"You can now use source data for pre-training and target data for fine-tuning!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Loading\n",
      "========================================\n",
      "Main dataset (R5):\n",
      "Location: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_5\n",
      "Recordings: 745\n",
      "   ðŸ”¬ First recording:\n",
      "      - Channels: 129\n",
      "      - Duration: 197.0s\n",
      "      - Sampling rate: 100.0 Hz\n",
      "      - Channel names: ['E1', 'E2', 'E3', 'E4', 'E5']...\n",
      "\n",
      "ðŸ”„ Testing loading different release...\n",
      "Loading Release R1 from: /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
      "\n",
      "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
      "-------------------------------------------------------\n",
      "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
      "  - Downsampled from 500Hz to 100Hz\n",
      "  - Bandpass filtered (0.5â€“50 Hz)\n",
      "\n",
      "For full preprocessing details, see:\n",
      "  https://github.com/eeg2025/downsample-datasets\n",
      "\n",
      "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
      "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
      "\n",
      "\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 293 recordings from Release R1\n",
      "âœ… Successfully loaded Release R1: 293 recordings\n",
      "\n",
      "ðŸŽ‰ Data loading test complete!\n",
      "ðŸ’¡ Your notebook is now ready to work with your downloaded data structure!\n"
     ]
    }
   ],
   "source": [
    "# TEST DATA LOADING: Verify everything works with your folder structure\n",
    "print(\"Testing Data Loading\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Test the main dataset\n",
    "print(f\"Main dataset (R{RELEASE_ID}):\")\n",
    "print(f\"Location: {RELEASE_DIR.resolve()}\")\n",
    "print(f\"Recordings: {len(dataset_ccd.datasets)}\")\n",
    "\n",
    "if len(dataset_ccd.datasets) > 0:\n",
    "    # Show info about first recording\n",
    "    raw = dataset_ccd.datasets[0].raw\n",
    "    print(f\"   ðŸ”¬ First recording:\")\n",
    "    print(f\"      - Channels: {len(raw.ch_names)}\")\n",
    "    print(f\"      - Duration: {raw.times[-1]:.1f}s\")\n",
    "    print(f\"      - Sampling rate: {raw.info['sfreq']} Hz\")\n",
    "    print(f\"      - Channel names: {raw.ch_names[:5]}...\")  # First 5 channels\n",
    "\n",
    "# Test loading a different release\n",
    "print(f\"\\nðŸ”„ Testing loading different release...\")\n",
    "try:\n",
    "    test_release = 1 if RELEASE_ID != 1 else 2  # Test with a different release\n",
    "    test_dataset = load_release_data(test_release, mini=False)\n",
    "    print(f\"âœ… Successfully loaded Release R{test_release}: {len(test_dataset.datasets)} recordings\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load test release: {e}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Data loading test complete!\")\n",
    "print(f\"ðŸ’¡ Your notebook is now ready to work with your downloaded data structure!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SadqlJ02D3Lo",
    "outputId": "aea3c99f-5aff-4282-d0b2-022d43b3f88f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: braindecode in ./eeg_env/lib/python3.13/site-packages (1.2.0)\n",
      "Requirement already satisfied: mne>=1.10.0 in ./eeg_env/lib/python3.13/site-packages (from braindecode) (1.10.1)\n",
      "Requirement already satisfied: mne_bids>=0.16 in ./eeg_env/lib/python3.13/site-packages (from braindecode) (0.17.0)\n",
      "Requirement already satisfied: numpy in ./eeg_env/lib/python3.13/site-packages (from braindecode) (2.3.3)\n",
      "Requirement already satisfied: pandas in ./eeg_env/lib/python3.13/site-packages (from braindecode) (2.3.3)\n",
      "Requirement already satisfied: scipy in ./eeg_env/lib/python3.13/site-packages (from braindecode) (1.16.2)\n",
      "Requirement already satisfied: matplotlib in ./eeg_env/lib/python3.13/site-packages (from braindecode) (3.10.6)\n",
      "Requirement already satisfied: h5py in ./eeg_env/lib/python3.13/site-packages (from braindecode) (3.14.0)\n",
      "Requirement already satisfied: skorch>=1.2.0 in ./eeg_env/lib/python3.13/site-packages (from braindecode) (1.2.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.0 in ./eeg_env/lib/python3.13/site-packages (from braindecode) (2.8.0)\n",
      "Requirement already satisfied: torchaudio<3.0,>=2.0 in ./eeg_env/lib/python3.13/site-packages (from braindecode) (2.8.0)\n",
      "Requirement already satisfied: einops in ./eeg_env/lib/python3.13/site-packages (from braindecode) (0.8.1)\n",
      "Requirement already satisfied: joblib in ./eeg_env/lib/python3.13/site-packages (from braindecode) (1.5.2)\n",
      "Requirement already satisfied: torchinfo~=1.8 in ./eeg_env/lib/python3.13/site-packages (from braindecode) (1.8.0)\n",
      "Requirement already satisfied: wfdb in ./eeg_env/lib/python3.13/site-packages (from braindecode) (4.3.0)\n",
      "Requirement already satisfied: linear_attention_transformer in ./eeg_env/lib/python3.13/site-packages (from braindecode) (0.19.1)\n",
      "Requirement already satisfied: docstring_inheritance in ./eeg_env/lib/python3.13/site-packages (from braindecode) (2.2.2)\n",
      "Requirement already satisfied: filelock in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode) (2025.9.0)\n",
      "Requirement already satisfied: decorator in ./eeg_env/lib/python3.13/site-packages (from mne>=1.10.0->braindecode) (5.2.1)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in ./eeg_env/lib/python3.13/site-packages (from mne>=1.10.0->braindecode) (0.4)\n",
      "Requirement already satisfied: packaging in ./eeg_env/lib/python3.13/site-packages (from mne>=1.10.0->braindecode) (25.0)\n",
      "Requirement already satisfied: pooch>=1.5 in ./eeg_env/lib/python3.13/site-packages (from mne>=1.10.0->braindecode) (1.8.2)\n",
      "Requirement already satisfied: tqdm in ./eeg_env/lib/python3.13/site-packages (from mne>=1.10.0->braindecode) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./eeg_env/lib/python3.13/site-packages (from pooch>=1.5->mne>=1.10.0->braindecode) (4.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./eeg_env/lib/python3.13/site-packages (from pooch>=1.5->mne>=1.10.0->braindecode) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in ./eeg_env/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib->braindecode) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./eeg_env/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.10.0->braindecode) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./eeg_env/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.10.0->braindecode) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./eeg_env/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.10.0->braindecode) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./eeg_env/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.10.0->braindecode) (2025.10.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in ./eeg_env/lib/python3.13/site-packages (from skorch>=1.2.0->braindecode) (1.7.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in ./eeg_env/lib/python3.13/site-packages (from skorch>=1.2.0->braindecode) (0.9.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./eeg_env/lib/python3.13/site-packages (from scikit-learn>=0.22.0->skorch>=1.2.0->braindecode) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./eeg_env/lib/python3.13/site-packages (from sympy>=1.13.3->torch<3.0,>=2.0->braindecode) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./eeg_env/lib/python3.13/site-packages (from jinja2->torch<3.0,>=2.0->braindecode) (3.0.3)\n",
      "Requirement already satisfied: axial-positional-embedding in ./eeg_env/lib/python3.13/site-packages (from linear_attention_transformer->braindecode) (0.3.12)\n",
      "Requirement already satisfied: linformer>=0.1.0 in ./eeg_env/lib/python3.13/site-packages (from linear_attention_transformer->braindecode) (0.2.3)\n",
      "Requirement already satisfied: local-attention in ./eeg_env/lib/python3.13/site-packages (from linear_attention_transformer->braindecode) (1.11.2)\n",
      "Requirement already satisfied: product-key-memory>=0.1.5 in ./eeg_env/lib/python3.13/site-packages (from linear_attention_transformer->braindecode) (0.2.11)\n",
      "Requirement already satisfied: colt5-attention>=0.10.14 in ./eeg_env/lib/python3.13/site-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode) (0.11.1)\n",
      "Requirement already satisfied: hyper-connections>=0.1.8 in ./eeg_env/lib/python3.13/site-packages (from local-attention->linear_attention_transformer->braindecode) (0.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./eeg_env/lib/python3.13/site-packages (from pandas->braindecode) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./eeg_env/lib/python3.13/site-packages (from pandas->braindecode) (2025.2)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in ./eeg_env/lib/python3.13/site-packages (from wfdb->braindecode) (3.13.0)\n",
      "Requirement already satisfied: soundfile>=0.10.0 in ./eeg_env/lib/python3.13/site-packages (from wfdb->braindecode) (0.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./eeg_env/lib/python3.13/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./eeg_env/lib/python3.13/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.22.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./eeg_env/lib/python3.13/site-packages (from soundfile>=0.10.0->wfdb->braindecode) (2.0.0)\n",
      "Requirement already satisfied: pycparser in ./eeg_env/lib/python3.13/site-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->braindecode) (2.23)\n",
      "Requirement already satisfied: eegdash in ./eeg_env/lib/python3.13/site-packages (0.3.8)\n",
      "Requirement already satisfied: braindecode>=1.0 in ./eeg_env/lib/python3.13/site-packages (from eegdash) (1.2.0)\n",
      "Requirement already satisfied: mne_bids>=0.16.0 in ./eeg_env/lib/python3.13/site-packages (from eegdash) (0.17.0)\n",
      "Requirement already satisfied: numba in ./eeg_env/lib/python3.13/site-packages (from eegdash) (0.62.1)\n",
      "Requirement already satisfied: numpy in ./eeg_env/lib/python3.13/site-packages (from eegdash) (2.3.3)\n",
      "Requirement already satisfied: pandas in ./eeg_env/lib/python3.13/site-packages (from eegdash) (2.3.3)\n",
      "Requirement already satisfied: pybids in ./eeg_env/lib/python3.13/site-packages (from eegdash) (0.20.0)\n",
      "Requirement already satisfied: pymongo in ./eeg_env/lib/python3.13/site-packages (from eegdash) (4.15.3)\n",
      "Requirement already satisfied: python-dotenv in ./eeg_env/lib/python3.13/site-packages (from eegdash) (1.1.1)\n",
      "Requirement already satisfied: s3fs in ./eeg_env/lib/python3.13/site-packages (from eegdash) (2025.9.0)\n",
      "Requirement already satisfied: scipy in ./eeg_env/lib/python3.13/site-packages (from eegdash) (1.16.2)\n",
      "Requirement already satisfied: tqdm in ./eeg_env/lib/python3.13/site-packages (from eegdash) (4.67.1)\n",
      "Requirement already satisfied: xarray in ./eeg_env/lib/python3.13/site-packages (from eegdash) (2025.10.1)\n",
      "Requirement already satisfied: h5io>=0.2.4 in ./eeg_env/lib/python3.13/site-packages (from eegdash) (0.2.5)\n",
      "Requirement already satisfied: pymatreader in ./eeg_env/lib/python3.13/site-packages (from eegdash) (1.1.0)\n",
      "Requirement already satisfied: eeglabio in ./eeg_env/lib/python3.13/site-packages (from eegdash) (0.1.2)\n",
      "Requirement already satisfied: tabulate in ./eeg_env/lib/python3.13/site-packages (from eegdash) (0.9.0)\n",
      "Requirement already satisfied: docstring_inheritance in ./eeg_env/lib/python3.13/site-packages (from eegdash) (2.2.2)\n",
      "Requirement already satisfied: mne>=1.10.0 in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (1.10.1)\n",
      "Requirement already satisfied: matplotlib in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (3.10.6)\n",
      "Requirement already satisfied: h5py in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (3.14.0)\n",
      "Requirement already satisfied: skorch>=1.2.0 in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (1.2.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.0 in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (2.8.0)\n",
      "Requirement already satisfied: torchaudio<3.0,>=2.0 in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (2.8.0)\n",
      "Requirement already satisfied: einops in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (0.8.1)\n",
      "Requirement already satisfied: joblib in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (1.5.2)\n",
      "Requirement already satisfied: torchinfo~=1.8 in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (1.8.0)\n",
      "Requirement already satisfied: wfdb in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (4.3.0)\n",
      "Requirement already satisfied: linear_attention_transformer in ./eeg_env/lib/python3.13/site-packages (from braindecode>=1.0->eegdash) (0.19.1)\n",
      "Requirement already satisfied: filelock in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./eeg_env/lib/python3.13/site-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (2025.9.0)\n",
      "Requirement already satisfied: decorator in ./eeg_env/lib/python3.13/site-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (5.2.1)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in ./eeg_env/lib/python3.13/site-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (0.4)\n",
      "Requirement already satisfied: packaging in ./eeg_env/lib/python3.13/site-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (25.0)\n",
      "Requirement already satisfied: pooch>=1.5 in ./eeg_env/lib/python3.13/site-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (1.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode>=1.0->eegdash) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode>=1.0->eegdash) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode>=1.0->eegdash) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode>=1.0->eegdash) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode>=1.0->eegdash) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode>=1.0->eegdash) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./eeg_env/lib/python3.13/site-packages (from matplotlib->braindecode>=1.0->eegdash) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./eeg_env/lib/python3.13/site-packages (from pooch>=1.5->mne>=1.10.0->braindecode>=1.0->eegdash) (4.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./eeg_env/lib/python3.13/site-packages (from pooch>=1.5->mne>=1.10.0->braindecode>=1.0->eegdash) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in ./eeg_env/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib->braindecode>=1.0->eegdash) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./eeg_env/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.10.0->braindecode>=1.0->eegdash) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./eeg_env/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.10.0->braindecode>=1.0->eegdash) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./eeg_env/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.10.0->braindecode>=1.0->eegdash) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./eeg_env/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.10.0->braindecode>=1.0->eegdash) (2025.10.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in ./eeg_env/lib/python3.13/site-packages (from skorch>=1.2.0->braindecode>=1.0->eegdash) (1.7.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./eeg_env/lib/python3.13/site-packages (from scikit-learn>=0.22.0->skorch>=1.2.0->braindecode>=1.0->eegdash) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./eeg_env/lib/python3.13/site-packages (from sympy>=1.13.3->torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./eeg_env/lib/python3.13/site-packages (from jinja2->torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.0.3)\n",
      "Requirement already satisfied: axial-positional-embedding in ./eeg_env/lib/python3.13/site-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.3.12)\n",
      "Requirement already satisfied: linformer>=0.1.0 in ./eeg_env/lib/python3.13/site-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.3)\n",
      "Requirement already satisfied: local-attention in ./eeg_env/lib/python3.13/site-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (1.11.2)\n",
      "Requirement already satisfied: product-key-memory>=0.1.5 in ./eeg_env/lib/python3.13/site-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.11)\n",
      "Requirement already satisfied: colt5-attention>=0.10.14 in ./eeg_env/lib/python3.13/site-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode>=1.0->eegdash) (0.11.1)\n",
      "Requirement already satisfied: hyper-connections>=0.1.8 in ./eeg_env/lib/python3.13/site-packages (from local-attention->linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.1)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in ./eeg_env/lib/python3.13/site-packages (from numba->eegdash) (0.45.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./eeg_env/lib/python3.13/site-packages (from pandas->eegdash) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./eeg_env/lib/python3.13/site-packages (from pandas->eegdash) (2025.2)\n",
      "Requirement already satisfied: nibabel>=4.0 in ./eeg_env/lib/python3.13/site-packages (from pybids->eegdash) (5.3.2)\n",
      "Requirement already satisfied: formulaic>=0.3 in ./eeg_env/lib/python3.13/site-packages (from pybids->eegdash) (1.2.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.31 in ./eeg_env/lib/python3.13/site-packages (from pybids->eegdash) (2.0.43)\n",
      "Requirement already satisfied: bids-validator>=1.14.7 in ./eeg_env/lib/python3.13/site-packages (from pybids->eegdash) (1.14.7.post0)\n",
      "Requirement already satisfied: num2words>=0.5.10 in ./eeg_env/lib/python3.13/site-packages (from pybids->eegdash) (0.5.14)\n",
      "Requirement already satisfied: click>=8.0 in ./eeg_env/lib/python3.13/site-packages (from pybids->eegdash) (8.3.0)\n",
      "Requirement already satisfied: universal_pathlib>=0.2.2 in ./eeg_env/lib/python3.13/site-packages (from pybids->eegdash) (0.3.3)\n",
      "Requirement already satisfied: frozendict>=2.3 in ./eeg_env/lib/python3.13/site-packages (from pybids->eegdash) (2.4.6)\n",
      "Requirement already satisfied: bidsschematools>=0.10 in ./eeg_env/lib/python3.13/site-packages (from bids-validator>=1.14.7->pybids->eegdash) (1.1.0)\n",
      "Requirement already satisfied: acres in ./eeg_env/lib/python3.13/site-packages (from bidsschematools>=0.10->bids-validator>=1.14.7->pybids->eegdash) (0.5.0)\n",
      "Requirement already satisfied: pyyaml in ./eeg_env/lib/python3.13/site-packages (from bidsschematools>=0.10->bids-validator>=1.14.7->pybids->eegdash) (6.0.3)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in ./eeg_env/lib/python3.13/site-packages (from formulaic>=0.3->pybids->eegdash) (1.3.0)\n",
      "Requirement already satisfied: narwhals>=1.17 in ./eeg_env/lib/python3.13/site-packages (from formulaic>=0.3->pybids->eegdash) (2.7.0)\n",
      "Requirement already satisfied: wrapt>=1.17.0rc1 in ./eeg_env/lib/python3.13/site-packages (from formulaic>=0.3->pybids->eegdash) (1.17.3)\n",
      "Requirement already satisfied: docopt>=0.6.2 in ./eeg_env/lib/python3.13/site-packages (from num2words>=0.5.10->pybids->eegdash) (0.6.2)\n",
      "Requirement already satisfied: pathlib-abc==0.5.1 in ./eeg_env/lib/python3.13/site-packages (from universal_pathlib>=0.2.2->pybids->eegdash) (0.5.1)\n",
      "Requirement already satisfied: xmltodict in ./eeg_env/lib/python3.13/site-packages (from pymatreader->eegdash) (1.0.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in ./eeg_env/lib/python3.13/site-packages (from pymongo->eegdash) (2.8.0)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in ./eeg_env/lib/python3.13/site-packages (from s3fs->eegdash) (2.24.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./eeg_env/lib/python3.13/site-packages (from s3fs->eegdash) (3.13.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in ./eeg_env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.40.46,>=1.40.37 in ./eeg_env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.40.45)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./eeg_env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in ./eeg_env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (6.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./eeg_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.8.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./eeg_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.22.0)\n",
      "Requirement already satisfied: soundfile>=0.10.0 in ./eeg_env/lib/python3.13/site-packages (from wfdb->braindecode>=1.0->eegdash) (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in ./eeg_env/lib/python3.13/site-packages (from soundfile>=0.10.0->wfdb->braindecode>=1.0->eegdash) (2.0.0)\n",
      "Requirement already satisfied: pycparser in ./eeg_env/lib/python3.13/site-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->braindecode>=1.0->eegdash) (2.23)\n"
     ]
    }
   ],
   "source": [
    "#@title â–¶ï¸ Install additional required packages for colab\n",
    "!pip install braindecode\n",
    "!pip install eegdash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RD9B9MWEXVQ0"
   },
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# DATA_DIR = Path(\"data\")\n",
    "# DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# from eegdash.dataset import EEGChallengeDataset\n",
    "\n",
    "# dataset_ccd = EEGChallengeDataset(task=\"contrastChangeDetection\",\n",
    "#                                   release=\"R5\", cache_dir=DATA_DIR,\n",
    "#                                   mini=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r9SgxLb8Aht-"
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "raws = Parallel(n_jobs=-1)(\n",
    "    delayed(lambda d: d.raw)(d) for d in dataset_ccd.datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h_-gE1EgAht_"
   },
   "outputs": [],
   "source": [
    "#@title â–¶ï¸ Run this first to get all the utils functions for the epoching\n",
    "from braindecode.datasets import BaseConcatDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.002\n",
    "weight_decay = 0.001\n",
    "n_epochs = 25\n",
    "early_stopping_patience = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxWITGyLQhrM",
    "outputId": "c6899051-c9ad-4557-bc9d-77ccfa2d0839"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Recording /Users/kimberly/Documents/ESE5380/eeg_challenge/data/release_5/ds005509-bdf/sub-NDARVH153RE7/eeg/sub-NDARVH153RE7_task-contrastChangeDetection_run-1_eeg.bdf does not contain event 'stimulus_anchor'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
      "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import preprocess, Preprocessor, create_windows_from_events\n",
    "from eegdash.hbn.windows import (\n",
    "    annotate_trials_with_target,\n",
    "    add_aux_anchors,\n",
    "    add_extras_columns,\n",
    "    keep_only_recordings_with,\n",
    ")\n",
    "\n",
    "EPOCH_LEN_S = 2.0\n",
    "SFREQ = 100 # by definition here\n",
    "\n",
    "transformation_offline = [\n",
    "    Preprocessor(\n",
    "        annotate_trials_with_target,\n",
    "        target_field=\"rt_from_stimulus\", epoch_length=EPOCH_LEN_S,\n",
    "        require_stimulus=True, require_response=True,\n",
    "        apply_on_array=False,\n",
    "    ),\n",
    "    Preprocessor(add_aux_anchors, apply_on_array=False),\n",
    "]\n",
    "preprocess(dataset_ccd, transformation_offline, n_jobs=1)\n",
    "\n",
    "ANCHOR = \"stimulus_anchor\"\n",
    "\n",
    "SHIFT_AFTER_STIM = 0.5\n",
    "WINDOW_LEN       = 2.0\n",
    "\n",
    "# Keep only recordings that actually contain stimulus anchors\n",
    "dataset = keep_only_recordings_with(ANCHOR, dataset_ccd)\n",
    "\n",
    "# Create single-interval windows (stim-locked, long enough to include the response)\n",
    "single_windows = create_windows_from_events(\n",
    "    dataset,\n",
    "    mapping={ANCHOR: 0},\n",
    "    trial_start_offset_samples=int(SHIFT_AFTER_STIM * SFREQ),                 # +0.5 s\n",
    "    trial_stop_offset_samples=int((SHIFT_AFTER_STIM + WINDOW_LEN) * SFREQ),   # +2.5 s\n",
    "    window_size_samples=int(EPOCH_LEN_S * SFREQ),\n",
    "    window_stride_samples=SFREQ,\n",
    "    preload=True,\n",
    ")\n",
    "\n",
    "# Injecting metadata into the extra mne annotation.\n",
    "single_windows = add_extras_columns(\n",
    "    single_windows,\n",
    "    dataset,\n",
    "    desc=ANCHOR,\n",
    "    keys=(\"target\", \"rt_from_stimulus\", \"rt_from_trialstart\",\n",
    "          \"stimulus_onset\", \"response_onset\", \"correct\", \"response_type\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "toaJHKWyAhuB"
   },
   "outputs": [],
   "source": [
    "# for each windows, we can extract the metainformation using:\n",
    "\n",
    "meta_information = single_windows.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "4fRiTMRuAhuB",
    "outputId": "007e4f9b-ec31-4c14-bfcc-7b4669e95e97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_window_in_trial</th>\n",
       "      <th>i_start_in_trial</th>\n",
       "      <th>i_stop_in_trial</th>\n",
       "      <th>target</th>\n",
       "      <th>rt_from_stimulus</th>\n",
       "      <th>rt_from_trialstart</th>\n",
       "      <th>stimulus_onset</th>\n",
       "      <th>response_onset</th>\n",
       "      <th>correct</th>\n",
       "      <th>response_type</th>\n",
       "      <th>...</th>\n",
       "      <th>thepresent</th>\n",
       "      <th>diaryofawimpykid</th>\n",
       "      <th>contrastchangedetection_1</th>\n",
       "      <th>contrastchangedetection_2</th>\n",
       "      <th>contrastchangedetection_3</th>\n",
       "      <th>surroundsupp_1</th>\n",
       "      <th>surroundsupp_2</th>\n",
       "      <th>seqlearning6target</th>\n",
       "      <th>seqlearning8target</th>\n",
       "      <th>symbolsearch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2241</td>\n",
       "      <td>2441</td>\n",
       "      <td>1.486</td>\n",
       "      <td>1.486</td>\n",
       "      <td>4.484</td>\n",
       "      <td>21.906</td>\n",
       "      <td>23.392</td>\n",
       "      <td>0</td>\n",
       "      <td>right_buttonPress</td>\n",
       "      <td>...</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>unavailable</td>\n",
       "      <td>caution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3284</td>\n",
       "      <td>3484</td>\n",
       "      <td>1.562</td>\n",
       "      <td>1.562</td>\n",
       "      <td>4.57</td>\n",
       "      <td>32.34</td>\n",
       "      <td>33.902</td>\n",
       "      <td>1</td>\n",
       "      <td>right_buttonPress</td>\n",
       "      <td>...</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>unavailable</td>\n",
       "      <td>caution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4322</td>\n",
       "      <td>4522</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.51</td>\n",
       "      <td>5.606</td>\n",
       "      <td>42.724</td>\n",
       "      <td>44.234</td>\n",
       "      <td>1</td>\n",
       "      <td>right_buttonPress</td>\n",
       "      <td>...</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>unavailable</td>\n",
       "      <td>caution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5360</td>\n",
       "      <td>5560</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>3.972</td>\n",
       "      <td>53.096</td>\n",
       "      <td>54.066</td>\n",
       "      <td>0</td>\n",
       "      <td>right_buttonPress</td>\n",
       "      <td>...</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>unavailable</td>\n",
       "      <td>caution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5714</td>\n",
       "      <td>5914</td>\n",
       "      <td>1.172</td>\n",
       "      <td>1.172</td>\n",
       "      <td>3.07</td>\n",
       "      <td>56.644</td>\n",
       "      <td>57.816</td>\n",
       "      <td>1</td>\n",
       "      <td>right_buttonPress</td>\n",
       "      <td>...</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>available</td>\n",
       "      <td>unavailable</td>\n",
       "      <td>caution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_window_in_trial  i_start_in_trial  i_stop_in_trial  target  \\\n",
       "0                  0              2241             2441   1.486   \n",
       "1                  0              3284             3484   1.562   \n",
       "2                  0              4322             4522    1.51   \n",
       "3                  0              5360             5560    0.97   \n",
       "4                  0              5714             5914   1.172   \n",
       "\n",
       "   rt_from_stimulus  rt_from_trialstart  stimulus_onset  response_onset  \\\n",
       "0             1.486               4.484          21.906          23.392   \n",
       "1             1.562                4.57           32.34          33.902   \n",
       "2              1.51               5.606          42.724          44.234   \n",
       "3              0.97               3.972          53.096          54.066   \n",
       "4             1.172                3.07          56.644          57.816   \n",
       "\n",
       "   correct      response_type  ... thepresent  diaryofawimpykid  \\\n",
       "0        0  right_buttonPress  ...  available         available   \n",
       "1        1  right_buttonPress  ...  available         available   \n",
       "2        1  right_buttonPress  ...  available         available   \n",
       "3        0  right_buttonPress  ...  available         available   \n",
       "4        1  right_buttonPress  ...  available         available   \n",
       "\n",
       "  contrastchangedetection_1  contrastchangedetection_2  \\\n",
       "0                 available                  available   \n",
       "1                 available                  available   \n",
       "2                 available                  available   \n",
       "3                 available                  available   \n",
       "4                 available                  available   \n",
       "\n",
       "  contrastchangedetection_3 surroundsupp_1  surroundsupp_2 seqlearning6target  \\\n",
       "0                 available      available       available          available   \n",
       "1                 available      available       available          available   \n",
       "2                 available      available       available          available   \n",
       "3                 available      available       available          available   \n",
       "4                 available      available       available          available   \n",
       "\n",
       "  seqlearning8target  symbolsearch  \n",
       "0        unavailable       caution  \n",
       "1        unavailable       caution  \n",
       "2        unavailable       caution  \n",
       "3        unavailable       caution  \n",
       "4        unavailable       caution  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Wip6B3jUAhuB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "valid_frac = 0.1\n",
    "test_frac = 0.1\n",
    "seed = 2025\n",
    "\n",
    "subjects = meta_information[\"subject\"].unique()\n",
    "sub_rm = [\"NDARWV769JM7\", \"NDARME789TD2\", \"NDARUA442ZVF\", \"NDARJP304NK1\",\n",
    "          \"NDARTY128YLU\", \"NDARDW550GU6\", \"NDARLD243KRE\", \"NDARUJ292JXV\", \"NDARBA381JGH\"]\n",
    "subjects = [s for s in subjects if s not in sub_rm]\n",
    "\n",
    "train_subj, valid_test_subject = train_test_split(\n",
    "    subjects, test_size=(valid_frac + test_frac), random_state=check_random_state(seed), shuffle=True\n",
    ")\n",
    "\n",
    "valid_subj, test_subj = train_test_split(\n",
    "    valid_test_subject, test_size=test_frac, random_state=check_random_state(seed + 1), shuffle=True\n",
    ")\n",
    "# sanity check\n",
    "assert (set(valid_subj) | set(test_subj) | set(train_subj)) == set(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlH0HB6oAhuC",
    "outputId": "ef7f3b63-d08e-42b4-bfd1-e1d600bf6eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in each split in the minirelease\n",
      "Train:\t12100\n",
      "Valid:\t2714\n",
      "Test:\t330\n"
     ]
    }
   ],
   "source": [
    "# and finally using braindecode split function, we can do:\n",
    "subject_split = single_windows.split(\"subject\")\n",
    "\n",
    "train_set = []\n",
    "valid_set = []\n",
    "test_set = []\n",
    "\n",
    "for s in subject_split:\n",
    "    if s in train_subj:\n",
    "        train_set.append(subject_split[s])\n",
    "    elif s in valid_subj:\n",
    "        valid_set.append(subject_split[s])\n",
    "    elif s in test_subj:\n",
    "        test_set.append(subject_split[s])\n",
    "\n",
    "train_set = BaseConcatDataset(train_set)\n",
    "valid_set = BaseConcatDataset(valid_set)\n",
    "test_set = BaseConcatDataset(test_set)\n",
    "\n",
    "print(\"Number of examples in each split in the minirelease\")\n",
    "print(f\"Train:\\t{len(train_set)}\")\n",
    "print(f\"Valid:\\t{len(valid_set)}\")\n",
    "print(f\"Test:\\t{len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uTIL4-0NQhrN"
   },
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 1 # We are using a single worker, but you can increase this for faster data loading\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3JjNTe0QhrN"
   },
   "source": [
    "## Building the deep learning model\n",
    "\n",
    "For neural network models, **to start**, we suggest using [braindecode models](https://braindecode.org/1.2/models/models_table.html) zoo. We have implemented several different models for decoding the brain timeseries.\n",
    "\n",
    "Your team's responsibility is to develop a PyTorch module that receives the three-dimensional (`batch`, `n_chans`, `n_times`) input and outputs the contrastive response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_UiLmm3AhuE",
    "outputId": "4c79735b-1d46-4d9b-a210-52eb2548c000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATCNet                  AttentionBaseNet        AttnSleep             \n",
      "BDTCN                   BIOT                    CTNet                 \n",
      "ContraWR                Deep4Net                DeepSleepNet          \n",
      "EEGConformer            EEGITNet                EEGInceptionERP       \n",
      "EEGInceptionMI          EEGMiner                EEGNeX                \n",
      "EEGNet                  EEGSimpleConv           EEGTCNet              \n",
      "FBCNet                  FBLightConvNet          FBMSNet               \n",
      "IFNet                   Labram                  MSVTNet               \n",
      "SCCNet                  SPARCNet                ShallowFBCSPNet       \n",
      "SignalJEPA              SignalJEPA_Contextual   SignalJEPA_PostLocal  \n",
      "SignalJEPA_PreLocal     SincShallowNet          SleepStagerBlanco2020 \n",
      "SleepStagerChambon2018  SyncNet                 TIDNet                \n",
      "TSception               USleep                \n"
     ]
    }
   ],
   "source": [
    "from braindecode.models.util import models_dict\n",
    "\n",
    "names = sorted(models_dict)\n",
    "w = max(len(n) for n in names)\n",
    "\n",
    "for i in range(0, len(names), 3):\n",
    "    row = names[i:i+3]\n",
    "    print(\"  \".join(f\"{n:<{w}}\" for n in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                                      Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "==========================================================================================================================================================================\n",
      "EEGConformer (EEGConformer)                                            [1, 129, 200]             [1, 1]                    --                        --\n",
      "â”œâ”€_PatchEmbedding (patch_embedding): 1-1                               [1, 1, 129, 200]          [1, 7, 40]                --                        --\n",
      "â”‚    â””â”€Sequential (shallownet): 2-1                                    [1, 1, 129, 200]          [1, 40, 1, 7]             --                        --\n",
      "â”‚    â”‚    â””â”€Conv2d (0): 3-1                                            [1, 1, 129, 200]          [1, 40, 129, 176]         1,040                     [1, 25]\n",
      "â”‚    â”‚    â””â”€Conv2d (1): 3-2                                            [1, 40, 129, 176]         [1, 40, 1, 176]           206,440                   [129, 1]\n",
      "â”‚    â”‚    â””â”€BatchNorm2d (2): 3-3                                       [1, 40, 1, 176]           [1, 40, 1, 176]           80                        --\n",
      "â”‚    â”‚    â””â”€ELU (3): 3-4                                               [1, 40, 1, 176]           [1, 40, 1, 176]           --                        --\n",
      "â”‚    â”‚    â””â”€AvgPool2d (4): 3-5                                         [1, 40, 1, 176]           [1, 40, 1, 7]             --                        [1, 75]\n",
      "â”‚    â”‚    â””â”€Dropout (5): 3-6                                           [1, 40, 1, 7]             [1, 40, 1, 7]             --                        --\n",
      "â”‚    â””â”€Sequential (projection): 2-2                                    [1, 40, 1, 7]             [1, 7, 40]                --                        --\n",
      "â”‚    â”‚    â””â”€Conv2d (0): 3-7                                            [1, 40, 1, 7]             [1, 40, 1, 7]             1,640                     [1, 1]\n",
      "â”‚    â”‚    â””â”€Rearrange (1): 3-8                                         [1, 40, 1, 7]             [1, 7, 40]                --                        --\n",
      "â”œâ”€_TransformerEncoder (transformer): 1-2                               [1, 7, 40]                [1, 7, 40]                --                        --\n",
      "â”‚    â””â”€_TransformerEncoderBlock (0): 2-3                               [1, 7, 40]                [1, 7, 40]                --                        --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (0): 3-9                                      [1, 7, 40]                [1, 7, 40]                6,640                     --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (1): 3-10                                     [1, 7, 40]                [1, 7, 40]                13,080                    --\n",
      "â”‚    â””â”€_TransformerEncoderBlock (1): 2-4                               [1, 7, 40]                [1, 7, 40]                --                        --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (0): 3-11                                     [1, 7, 40]                [1, 7, 40]                6,640                     --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (1): 3-12                                     [1, 7, 40]                [1, 7, 40]                13,080                    --\n",
      "â”‚    â””â”€_TransformerEncoderBlock (2): 2-5                               [1, 7, 40]                [1, 7, 40]                --                        --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (0): 3-13                                     [1, 7, 40]                [1, 7, 40]                6,640                     --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (1): 3-14                                     [1, 7, 40]                [1, 7, 40]                13,080                    --\n",
      "â”‚    â””â”€_TransformerEncoderBlock (3): 2-6                               [1, 7, 40]                [1, 7, 40]                --                        --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (0): 3-15                                     [1, 7, 40]                [1, 7, 40]                6,640                     --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (1): 3-16                                     [1, 7, 40]                [1, 7, 40]                13,080                    --\n",
      "â”‚    â””â”€_TransformerEncoderBlock (4): 2-7                               [1, 7, 40]                [1, 7, 40]                --                        --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (0): 3-17                                     [1, 7, 40]                [1, 7, 40]                6,640                     --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (1): 3-18                                     [1, 7, 40]                [1, 7, 40]                13,080                    --\n",
      "â”‚    â””â”€_TransformerEncoderBlock (5): 2-8                               [1, 7, 40]                [1, 7, 40]                --                        --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (0): 3-19                                     [1, 7, 40]                [1, 7, 40]                6,640                     --\n",
      "â”‚    â”‚    â””â”€_ResidualAdd (1): 3-20                                     [1, 7, 40]                [1, 7, 40]                13,080                    --\n",
      "â”œâ”€_FullyConnected (fc): 1-3                                            [1, 7, 40]                [1, 32]                   --                        --\n",
      "â”‚    â””â”€Sequential (fc): 2-9                                            [1, 280]                  [1, 32]                   --                        --\n",
      "â”‚    â”‚    â””â”€Linear (0): 3-21                                           [1, 280]                  [1, 256]                  71,936                    --\n",
      "â”‚    â”‚    â””â”€ELU (1): 3-22                                              [1, 256]                  [1, 256]                  --                        --\n",
      "â”‚    â”‚    â””â”€Dropout (2): 3-23                                          [1, 256]                  [1, 256]                  --                        --\n",
      "â”‚    â”‚    â””â”€Linear (3): 3-24                                           [1, 256]                  [1, 32]                   8,224                     --\n",
      "â”‚    â”‚    â””â”€ELU (4): 3-25                                              [1, 32]                   [1, 32]                   --                        --\n",
      "â”‚    â”‚    â””â”€Dropout (5): 3-26                                          [1, 32]                   [1, 32]                   --                        --\n",
      "â”œâ”€Linear (final_layer): 1-4                                            [1, 32]                   [1, 1]                    33                        --\n",
      "==========================================================================================================================================================================\n",
      "Total params: 407,713\n",
      "Trainable params: 407,713\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 60.16\n",
      "==========================================================================================================================================================================\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 7.53\n",
      "Params size (MB): 1.63\n",
      "Estimated Total Size (MB): 9.26\n",
      "==========================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/ESE5380/eeg_challenge/eeg_env/lib/python3.13/site-packages/braindecode/models/eegconformer.py:227: UserWarning: This model has only been tested on no more than 64 channels. no guarantee to work with more channels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from braindecode.models import EEGConformer\n",
    "model2 = EEGConformer(n_chans=129, # 129 channels\n",
    "                n_outputs=1, # 1 output for regression\n",
    "                n_times=200, #2 seconds\n",
    "                sfreq=100,      # sample frequency 100 Hz\n",
    "                )\n",
    "\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(model2.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs - 1)\n",
    "# loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# patience = 5\n",
    "# min_delta = 1e-4\n",
    "# best_rmse = float(\"inf\")\n",
    "# epochs_no_improve = 0\n",
    "# best_state, best_epoch = None, None\n",
    "\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     print(f\"Epoch {epoch}/{n_epochs}: \", end=\"\")\n",
    "\n",
    "#     train_loss, train_rmse = train_one_epoch(\n",
    "#         train_loader, model2, loss_fn, optimizer, scheduler, epoch, device\n",
    "#     )\n",
    "#     val_loss, val_rmse = valid_model(test_loader, model2, loss_fn, device)\n",
    "\n",
    "#     print(\n",
    "#         f\"Train RMSE: {train_rmse:.6f}, \"\n",
    "#         f\"Average Train Loss: {train_loss:.6f}, \"\n",
    "#         f\"Val RMSE: {val_rmse:.6f}, \"\n",
    "#         f\"Average Val Loss: {val_loss:.6f}\"\n",
    "#     )\n",
    "\n",
    "#     if val_rmse < best_rmse - min_delta:\n",
    "#         best_rmse = val_rmse\n",
    "#         best_state = copy.deepcopy(model2.state_dict())\n",
    "#         best_epoch = epoch\n",
    "#         epochs_no_improve = 0\n",
    "#     else:\n",
    "#         epochs_no_improve += 1\n",
    "#         if epochs_no_improve >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch}. Best Val RMSE: {best_rmse:.6f} (epoch {best_epoch})\")\n",
    "#             break\n",
    "\n",
    "# if best_state is not None:\n",
    "#     model2.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEGConformer training with L1/L2 (ridge/lasso) sweep and plots\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import MSELoss\n",
    "from braindecode.models import EEGConformer\n",
    "\n",
    "# Assumes these exist from earlier cells:\n",
    "# - device\n",
    "# - train_loader, valid_loader (val == releases 5 & 6 per your split)\n",
    "# - train_one_epoch, valid_model\n",
    "\n",
    "def train_conformer_with_regularization(\n",
    "    model_type: str,\n",
    "    lam: float,\n",
    "    *,\n",
    "    n_epochs: int = 8,\n",
    "    lr: float = 1e-3,\n",
    "    print_batch_stats: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    model_type: 'ridge' -> L2 via optimizer weight_decay\n",
    "                'lasso' -> L1 penalty added to loss\n",
    "    lam: regularization strength\n",
    "    Returns (final_train_rmse, final_val_rmse) computed without reg terms.\n",
    "    \"\"\"\n",
    "    assert model_type in (\"ridge\", \"lasso\")\n",
    "\n",
    "    model = EEGConformer(n_chans=129, n_outputs=1, n_times=200, sfreq=100).to(device)\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    if model_type == \"ridge\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=lam)\n",
    "        add_l1 = False\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "        add_l1 = True\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs - 1)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # One epoch train; for L1 add |w| term\n",
    "        model.train()\n",
    "        for X, y, *_ in train_loader:\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, y)\n",
    "            if add_l1:\n",
    "                l1 = 0.0\n",
    "                for p in model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        l1 = l1 + p.abs().sum()\n",
    "                loss = loss + lam * l1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation (plain MSE)\n",
    "        val_loss, _ = valid_model(valid_loader, model, loss_fn, device, print_batch_stats=print_batch_stats)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Final train/val RMSE (no reg term) - compute RMSE = sqrt(MSE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t_sum_sq_err, t_n = 0.0, 0\n",
    "        for X, y, *_ in train_loader:\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            pred = model(X)\n",
    "            pred_flat = pred.view(-1)\n",
    "            y_flat = y.view(-1)\n",
    "            t_sum_sq_err += torch.sum((pred_flat - y_flat) ** 2).item()\n",
    "            t_n += y_flat.numel()\n",
    "\n",
    "        v_sum_sq_err, v_n = 0.0, 0\n",
    "        for X, y, *_ in valid_loader:\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            pred = model(X)\n",
    "            pred_flat = pred.view(-1)\n",
    "            y_flat = y.view(-1)\n",
    "            v_sum_sq_err += torch.sum((pred_flat - y_flat) ** 2).item()\n",
    "            v_n += y_flat.numel()\n",
    "\n",
    "    train_rmse = (t_sum_sq_err / max(t_n, 1)) ** 0.5\n",
    "    val_rmse = (v_sum_sq_err / max(v_n, 1)) ** 0.5\n",
    "    return train_rmse, val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Module\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_model(\n",
    "    dataloader: DataLoader,\n",
    "    model: Module,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    print_batch_stats: bool = True,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    sum_sq_err = 0.0\n",
    "    n_batches = len(dataloader)\n",
    "    n_samples = 0\n",
    "\n",
    "    iterator = tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=n_batches,\n",
    "        disable=not print_batch_stats\n",
    "    )\n",
    "\n",
    "    for batch_idx, batch in iterator:\n",
    "        X, y = batch[0], batch[1]\n",
    "        X, y = X.to(device).float(), y.to(device).float()\n",
    "        # casting X to float32\n",
    "\n",
    "        preds = model(X)\n",
    "        batch_loss = loss_fn(preds, y).item()\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        preds_flat = preds.detach().view(-1)\n",
    "        y_flat = y.detach().view(-1)\n",
    "        sum_sq_err += torch.sum((preds_flat - y_flat) ** 2).item()\n",
    "        n_samples += y_flat.numel()\n",
    "\n",
    "        if print_batch_stats:\n",
    "            running_rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
    "            iterator.set_description(\n",
    "                f\"Val Batch {batch_idx + 1}/{n_batches}, \"\n",
    "                f\"Loss: {batch_loss:.6f}, RMSE: {running_rmse:.6f}\"\n",
    "            )\n",
    "\n",
    "    avg_loss = total_loss / n_batches if n_batches else float(\"nan\")\n",
    "    rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
    "\n",
    "    print(f\"Val RMSE: {rmse:.6f}, Val Loss: {avg_loss:.6f}\\n\")\n",
    "    return avg_loss, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Sweep lambdas and plot\n",
    "# lambdas = [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2]\n",
    "\n",
    "# # Ridge (L2)\n",
    "# ridge_train, ridge_val = [], []\n",
    "# for lam in lambdas:\n",
    "#     tr, va = train_conformer_with_regularization(\"ridge\", lam, n_epochs=8, lr=1e-3)\n",
    "#     ridge_train.append(tr)\n",
    "#     ridge_val.append(va)\n",
    "\n",
    "# # Lasso (L1)\n",
    "# lasso_train, lasso_val = [], []\n",
    "# for lam in lambdas:\n",
    "#     tr, va = train_conformer_with_regularization(\"lasso\", lam, n_epochs=8, lr=1e-3)\n",
    "#     lasso_train.append(tr)\n",
    "#     lasso_val.append(va)\n",
    "\n",
    "# # Plots: two graphs, each with train and val vs lambda\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# # Ridge\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(lambdas, ridge_train, marker='o', label='Train Loss')\n",
    "# plt.plot(lambdas, ridge_val, marker='s', label='Val Loss')\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('Lambda (L2 weight decay)')\n",
    "# plt.ylabel('MSE Loss')\n",
    "# plt.title('EEGConformer Ridge: Train/Val Loss vs Lambda')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "# # Lasso\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(lambdas, lasso_train, marker='o', label='Train Loss')\n",
    "# plt.plot(lambdas, lasso_val, marker='s', label='Val Loss')\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('Lambda (L1 penalty)')\n",
    "# plt.ylabel('MSE Loss')\n",
    "# plt.title('EEGConformer Lasso: Train/Val Loss vs Lambda')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Ridge:\")\n",
    "# for lam, tr, va in zip(lambdas, ridge_train, ridge_val):\n",
    "#     print(f\"  {lam:>8}: train={tr:.6f}, val={va:.6f}\")\n",
    "# print(\"\\nLasso:\")\n",
    "# for lam, tr, va in zip(lambdas, lasso_train, lasso_val):\n",
    "#     print(f\"  {lam:>8}: train={tr:.6f}, val={va:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model2.state_dict(), \"eegconformer_reg.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = save_model_as_pt(\n",
    "#     model=model2,\n",
    "#     filename=\"eegconformer_final.pt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Sweep lambdas and plot\n",
    "# lambdas = [0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "\n",
    "# # Ridge (L2)\n",
    "# ridge_train, ridge_val = [], []\n",
    "# for lam in lambdas:\n",
    "#     tr, va = train_conformer_with_regularization(\"ridge\", lam, n_epochs=8, lr=1e-3)\n",
    "#     ridge_train.append(tr)\n",
    "#     ridge_val.append(va)\n",
    "\n",
    "# # Lasso (L1)\n",
    "# lasso_train, lasso_val = [], []\n",
    "# for lam in lambdas:\n",
    "#     tr, va = train_conformer_with_regularization(\"lasso\", lam, n_epochs=8, lr=1e-3)\n",
    "#     lasso_train.append(tr)\n",
    "#     lasso_val.append(va)\n",
    "\n",
    "# # Plots: two graphs, each with train and val vs lambda\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# # Ridge\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(lambdas, ridge_train, marker='o', label='Train Loss')\n",
    "# plt.plot(lambdas, ridge_val, marker='s', label='Val Loss')\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('Lambda (L2 weight decay)')\n",
    "# plt.ylabel('MSE Loss')\n",
    "# plt.title('EEGConformer Ridge: Train/Val Loss vs Lambda')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "# # Lasso\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(lambdas, lasso_train, marker='o', label='Train Loss')\n",
    "# plt.plot(lambdas, lasso_val, marker='s', label='Val Loss')\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('Lambda (L1 penalty)')\n",
    "# plt.ylabel('MSE Loss')\n",
    "# plt.title('EEGConformer Lasso: Train/Val Loss vs Lambda')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Ridge:\")\n",
    "# for lam, tr, va in zip(lambdas, ridge_train, ridge_val):\n",
    "#     print(f\"  {lam:>8}: train={tr:.6f}, val={va:.6f}\")\n",
    "# print(\"\\nLasso:\")\n",
    "# for lam, tr, va in zip(lambdas, lasso_train, lasso_val):\n",
    "#     print(f\"  {lam:>8}: train={tr:.6f}, val={va:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.419568, Val Loss: 0.175330\n",
      "\n",
      "Val RMSE: 0.407949, Val Loss: 0.164300\n",
      "\n",
      "Val RMSE: 0.422342, Val Loss: 0.174247\n",
      "\n",
      "Val RMSE: 0.417407, Val Loss: 0.171594\n",
      "\n",
      "Val RMSE: 0.404759, Val Loss: 0.161925\n",
      "\n",
      "Val RMSE: 3.351455, Val Loss: 11.180099\n",
      "\n",
      "Val RMSE: 0.460944, Val Loss: 0.213150\n",
      "\n",
      "Val RMSE: 0.419992, Val Loss: 0.172948\n",
      "\n",
      "Val RMSE: 0.561090, Val Loss: 0.318240\n",
      "\n",
      "Val RMSE: 0.495371, Val Loss: 0.247048\n",
      "\n",
      "Val RMSE: 0.419475, Val Loss: 0.173030\n",
      "\n",
      "Val RMSE: 0.501385, Val Loss: 0.252129\n",
      "\n",
      "Val RMSE: 8.865384, Val Loss: 78.470626\n",
      "\n",
      "Val RMSE: 0.451370, Val Loss: 0.198413\n",
      "\n",
      "Val RMSE: 0.443894, Val Loss: 0.195986\n",
      "\n",
      "Val RMSE: 0.482587, Val Loss: 0.226468\n",
      "\n",
      "Val RMSE: 0.448423, Val Loss: 0.200391\n",
      "\n",
      "Val RMSE: 0.438142, Val Loss: 0.190145\n",
      "\n",
      "Val RMSE: 0.561661, Val Loss: 0.306685\n",
      "\n",
      "Val RMSE: 0.425098, Val Loss: 0.179019\n",
      "\n",
      "Val RMSE: 0.486635, Val Loss: 0.237315\n",
      "\n",
      "Val RMSE: 0.446005, Val Loss: 0.194010\n",
      "\n",
      "Val RMSE: 0.443958, Val Loss: 0.192389\n",
      "\n",
      "Val RMSE: 0.446294, Val Loss: 0.193974\n",
      "\n",
      "Val RMSE: 0.444735, Val Loss: 0.193573\n",
      "\n",
      "Val RMSE: 0.456880, Val Loss: 0.207579\n",
      "\n",
      "Val RMSE: 0.440963, Val Loss: 0.191887\n",
      "\n",
      "Val RMSE: 0.433011, Val Loss: 0.183869\n",
      "\n",
      "Val RMSE: 0.451144, Val Loss: 0.201096\n",
      "\n",
      "Val RMSE: 0.455400, Val Loss: 0.202081\n",
      "\n",
      "Val RMSE: 4.218319, Val Loss: 17.729219\n",
      "\n",
      "Val RMSE: 1.967796, Val Loss: 3.839334\n",
      "\n",
      "Val RMSE: 0.639165, Val Loss: 0.397421\n",
      "\n",
      "Val RMSE: 0.427245, Val Loss: 0.179088\n",
      "\n",
      "Val RMSE: 0.596392, Val Loss: 0.358229\n",
      "\n",
      "Val RMSE: 0.647566, Val Loss: 0.422133\n",
      "\n",
      "Val RMSE: 0.514546, Val Loss: 0.261234\n",
      "\n",
      "Val RMSE: 0.531905, Val Loss: 0.277599\n",
      "\n",
      "Val RMSE: 0.489407, Val Loss: 0.237150\n",
      "\n",
      "Val RMSE: 0.623252, Val Loss: 0.381823\n",
      "\n",
      "Val RMSE: 0.485069, Val Loss: 0.234315\n",
      "\n",
      "Val RMSE: 0.654214, Val Loss: 0.429380\n",
      "\n",
      "Val RMSE: 0.722151, Val Loss: 0.519779\n",
      "\n",
      "Val RMSE: 0.876994, Val Loss: 0.756391\n",
      "\n",
      "Val RMSE: 0.540733, Val Loss: 0.290468\n",
      "\n",
      "Val RMSE: 0.487661, Val Loss: 0.236631\n",
      "\n",
      "Val RMSE: 0.481244, Val Loss: 0.226058\n",
      "\n",
      "Val RMSE: 0.591100, Val Loss: 0.346651\n",
      "\n",
      "Val RMSE: 0.569763, Val Loss: 0.314866\n",
      "\n",
      "Val RMSE: 0.517088, Val Loss: 0.260989\n",
      "\n",
      "Val RMSE: 0.765556, Val Loss: 0.567438\n",
      "\n",
      "Val RMSE: 0.456936, Val Loss: 0.204805\n",
      "\n",
      "Val RMSE: 0.705858, Val Loss: 0.482482\n",
      "\n",
      "Val RMSE: 0.480920, Val Loss: 0.232691\n",
      "\n",
      "Val RMSE: 0.415495, Val Loss: 0.170771\n",
      "\n",
      "Val RMSE: 1.241549, Val Loss: 1.520169\n",
      "\n",
      "Val RMSE: 0.425900, Val Loss: 0.179691\n",
      "\n",
      "Val RMSE: 0.912045, Val Loss: 0.841470\n",
      "\n",
      "Val RMSE: 0.602915, Val Loss: 0.367584\n",
      "\n",
      "Val RMSE: 0.526847, Val Loss: 0.277355\n",
      "\n",
      "Val RMSE: 0.507481, Val Loss: 0.250187\n",
      "\n",
      "Val RMSE: 3.417015, Val Loss: 11.621812\n",
      "\n",
      "Val RMSE: 0.538183, Val Loss: 0.292986\n",
      "\n",
      "Val RMSE: 0.506864, Val Loss: 0.258868\n",
      "\n",
      "Val RMSE: 0.497947, Val Loss: 0.250336\n",
      "\n",
      "Val RMSE: 0.534181, Val Loss: 0.287968\n",
      "\n",
      "Val RMSE: 0.642555, Val Loss: 0.418465\n",
      "\n",
      "Val RMSE: 0.425682, Val Loss: 0.180054\n",
      "\n",
      "Val RMSE: 0.492732, Val Loss: 0.244350\n",
      "\n",
      "Val RMSE: 0.546298, Val Loss: 0.290090\n",
      "\n",
      "Val RMSE: 0.420023, Val Loss: 0.173912\n",
      "\n",
      "Val RMSE: 0.429248, Val Loss: 0.183647\n",
      "\n",
      "Val RMSE: 0.429557, Val Loss: 0.183832\n",
      "\n",
      "Val RMSE: 0.472998, Val Loss: 0.217611\n",
      "\n",
      "Val RMSE: 0.415925, Val Loss: 0.169459\n",
      "\n",
      "Val RMSE: 0.448380, Val Loss: 0.200375\n",
      "\n",
      "Val RMSE: 0.790854, Val Loss: 0.633774\n",
      "\n",
      "Val RMSE: 0.877405, Val Loss: 0.754780\n",
      "\n",
      "Val RMSE: 0.422319, Val Loss: 0.176767\n",
      "\n",
      "Val RMSE: 0.502138, Val Loss: 0.252331\n",
      "\n",
      "Val RMSE: 0.699489, Val Loss: 0.494042\n",
      "\n",
      "Val RMSE: 0.480865, Val Loss: 0.224831\n",
      "\n",
      "Val RMSE: 0.442067, Val Loss: 0.191202\n",
      "\n",
      "Val RMSE: 0.485182, Val Loss: 0.233734\n",
      "\n",
      "Val RMSE: 0.510876, Val Loss: 0.262703\n",
      "\n",
      "Val RMSE: 0.423956, Val Loss: 0.176821\n",
      "\n",
      "Val RMSE: 0.487559, Val Loss: 0.236669\n",
      "\n",
      "Val RMSE: 0.466059, Val Loss: 0.213984\n",
      "\n",
      "Val RMSE: 0.460666, Val Loss: 0.206675\n",
      "\n",
      "Val RMSE: 0.620784, Val Loss: 0.386538\n",
      "\n",
      "Val RMSE: 0.434778, Val Loss: 0.185848\n",
      "\n",
      "Val RMSE: 0.547502, Val Loss: 0.296572\n",
      "\n",
      "Val RMSE: 0.590411, Val Loss: 0.347852\n",
      "\n",
      "Val RMSE: 0.452852, Val Loss: 0.202688\n",
      "\n",
      "Val RMSE: 0.505011, Val Loss: 0.249574\n",
      "\n",
      "Val RMSE: 0.550760, Val Loss: 0.299921\n",
      "\n",
      "Val RMSE: 0.476796, Val Loss: 0.221121\n",
      "\n",
      "Val RMSE: 0.486036, Val Loss: 0.231879\n",
      "\n",
      "Val RMSE: 0.664338, Val Loss: 0.428576\n",
      "\n",
      "Val RMSE: 0.455967, Val Loss: 0.203170\n",
      "\n",
      "Val RMSE: 0.474027, Val Loss: 0.218572\n",
      "\n",
      "Val RMSE: 0.448015, Val Loss: 0.198031\n",
      "\n",
      "Val RMSE: 0.854613, Val Loss: 0.715691\n",
      "\n",
      "Val RMSE: 0.446444, Val Loss: 0.199855\n",
      "\n",
      "Val RMSE: 0.426542, Val Loss: 0.178027\n",
      "\n",
      "Val RMSE: 0.507962, Val Loss: 0.260661\n",
      "\n",
      "Val RMSE: 0.489463, Val Loss: 0.241189\n",
      "\n",
      "Val RMSE: 0.489550, Val Loss: 0.241376\n",
      "\n",
      "Val RMSE: 2.884456, Val Loss: 8.277227\n",
      "\n",
      "Val RMSE: 0.416718, Val Loss: 0.172280\n",
      "\n",
      "Val RMSE: 0.419568, Val Loss: 0.174466\n",
      "\n",
      "Val RMSE: 0.590871, Val Loss: 0.352517\n",
      "\n",
      "Val RMSE: 0.595637, Val Loss: 0.359441\n",
      "\n",
      "Val RMSE: 0.416262, Val Loss: 0.170815\n",
      "\n",
      "Val RMSE: 0.475764, Val Loss: 0.220132\n",
      "\n",
      "Val RMSE: 0.521184, Val Loss: 0.264126\n",
      "\n",
      "Val RMSE: 0.417378, Val Loss: 0.173137\n",
      "\n",
      "Val RMSE: 0.436427, Val Loss: 0.190096\n",
      "\n",
      "Val RMSE: 0.451561, Val Loss: 0.204570\n",
      "\n",
      "Val RMSE: 0.427922, Val Loss: 0.178823\n",
      "\n",
      "Val RMSE: 0.456389, Val Loss: 0.209363\n",
      "\n",
      "Val RMSE: 0.437444, Val Loss: 0.186451\n",
      "\n",
      "Val RMSE: 0.429713, Val Loss: 0.184298\n",
      "\n",
      "Val RMSE: 0.448614, Val Loss: 0.201942\n",
      "\n",
      "Val RMSE: 0.412118, Val Loss: 0.166522\n",
      "\n",
      "Val RMSE: 0.559924, Val Loss: 0.317292\n",
      "\n",
      "Val RMSE: 0.439082, Val Loss: 0.187951\n",
      "\n",
      "Val RMSE: 0.519852, Val Loss: 0.272957\n",
      "\n",
      "Val RMSE: 0.461861, Val Loss: 0.213502\n",
      "\n",
      "Val RMSE: 0.419023, Val Loss: 0.173248\n",
      "\n",
      "Val RMSE: 0.423206, Val Loss: 0.175166\n",
      "\n",
      "Val RMSE: 0.421776, Val Loss: 0.174294\n",
      "\n",
      "Val RMSE: 0.433366, Val Loss: 0.183123\n",
      "\n",
      "Val RMSE: 0.420283, Val Loss: 0.175828\n",
      "\n",
      "Val RMSE: 0.529947, Val Loss: 0.282156\n",
      "\n",
      "Val RMSE: 0.458618, Val Loss: 0.204821\n",
      "\n",
      "Val RMSE: 0.500225, Val Loss: 0.243212\n",
      "\n",
      "Val RMSE: 0.627103, Val Loss: 0.395712\n",
      "\n",
      "Val RMSE: 0.455318, Val Loss: 0.204620\n",
      "\n",
      "Val RMSE: 0.445342, Val Loss: 0.197560\n",
      "\n",
      "Val RMSE: 0.469856, Val Loss: 0.219752\n",
      "\n",
      "Val RMSE: 0.465719, Val Loss: 0.214500\n",
      "\n",
      "Val RMSE: 0.477056, Val Loss: 0.226426\n",
      "\n",
      "Val RMSE: 0.415361, Val Loss: 0.169712\n",
      "\n",
      "Val RMSE: 0.433639, Val Loss: 0.185427\n",
      "\n",
      "Val RMSE: 0.522339, Val Loss: 0.264977\n",
      "\n",
      "Val RMSE: 0.507620, Val Loss: 0.250940\n",
      "\n",
      "Val RMSE: 0.560554, Val Loss: 0.304882\n",
      "\n",
      "Val RMSE: 0.504947, Val Loss: 0.247933\n",
      "\n",
      "Val RMSE: 0.427152, Val Loss: 0.178779\n",
      "\n",
      "Val RMSE: 0.439896, Val Loss: 0.192068\n",
      "\n",
      "Val RMSE: 0.459481, Val Loss: 0.205494\n",
      "\n",
      "Val RMSE: 0.587905, Val Loss: 0.336495\n",
      "\n",
      "Val RMSE: 0.420678, Val Loss: 0.174001\n",
      "\n",
      "Val RMSE: 0.439181, Val Loss: 0.188992\n",
      "\n",
      "Val RMSE: 0.454837, Val Loss: 0.205947\n",
      "\n",
      "Val RMSE: 0.429480, Val Loss: 0.182570\n",
      "\n",
      "Val RMSE: 0.438981, Val Loss: 0.191818\n",
      "\n",
      "Val RMSE: 0.432046, Val Loss: 0.185030\n",
      "\n",
      "Val RMSE: 0.435895, Val Loss: 0.189221\n",
      "\n",
      "Val RMSE: 0.423106, Val Loss: 0.177963\n",
      "\n",
      "Val RMSE: 0.424176, Val Loss: 0.179045\n",
      "\n",
      "Val RMSE: 0.427331, Val Loss: 0.181281\n",
      "\n",
      "Val RMSE: 0.455686, Val Loss: 0.202527\n",
      "\n",
      "Val RMSE: 0.594081, Val Loss: 0.356740\n",
      "\n",
      "Val RMSE: 0.434902, Val Loss: 0.187185\n",
      "\n",
      "Val RMSE: 0.410728, Val Loss: 0.165814\n",
      "\n",
      "Val RMSE: 0.459141, Val Loss: 0.209761\n",
      "\n",
      "Val RMSE: 0.449282, Val Loss: 0.199524\n",
      "\n",
      "Val RMSE: 0.422834, Val Loss: 0.174943\n",
      "\n",
      "Val RMSE: 0.428841, Val Loss: 0.180333\n",
      "\n",
      "Val RMSE: 0.487751, Val Loss: 0.239134\n",
      "\n",
      "Val RMSE: 0.481517, Val Loss: 0.225770\n",
      "\n",
      "Val RMSE: 0.427438, Val Loss: 0.182286\n",
      "\n",
      "Val RMSE: 0.429191, Val Loss: 0.180501\n",
      "\n",
      "Val RMSE: 0.407638, Val Loss: 0.164398\n",
      "\n",
      "Val RMSE: 0.422538, Val Loss: 0.178088\n",
      "\n",
      "Val RMSE: 0.418533, Val Loss: 0.171289\n",
      "\n",
      "Val RMSE: 0.436244, Val Loss: 0.190429\n",
      "\n",
      "Val RMSE: 0.627929, Val Loss: 0.384247\n",
      "\n",
      "Val RMSE: 0.433171, Val Loss: 0.187612\n",
      "\n",
      "Val RMSE: 1.131904, Val Loss: 1.262833\n",
      "\n",
      "Val RMSE: 0.518110, Val Loss: 0.261021\n",
      "\n",
      "Val RMSE: 0.419052, Val Loss: 0.173229\n",
      "\n",
      "Val RMSE: 0.454749, Val Loss: 0.201354\n",
      "\n",
      "Val RMSE: 0.525455, Val Loss: 0.279161\n",
      "\n",
      "Val RMSE: 0.425075, Val Loss: 0.178145\n",
      "\n",
      "Val RMSE: 0.416654, Val Loss: 0.170155\n",
      "\n",
      "Val RMSE: 0.526620, Val Loss: 0.269553\n",
      "\n",
      "Val RMSE: 0.627445, Val Loss: 0.397901\n",
      "\n",
      "Val RMSE: 0.521883, Val Loss: 0.274437\n",
      "\n",
      "Val RMSE: 0.419175, Val Loss: 0.172940\n",
      "\n",
      "Val RMSE: 0.430261, Val Loss: 0.182760\n",
      "\n",
      "Val RMSE: 0.431042, Val Loss: 0.184347\n",
      "\n",
      "Val RMSE: 0.494574, Val Loss: 0.246104\n",
      "\n",
      "Val RMSE: 0.439369, Val Loss: 0.193205\n",
      "\n",
      "Val RMSE: 0.448815, Val Loss: 0.196418\n",
      "\n",
      "Val RMSE: 0.439562, Val Loss: 0.188678\n",
      "\n",
      "Val RMSE: 0.455340, Val Loss: 0.207278\n",
      "\n",
      "Val RMSE: 0.453816, Val Loss: 0.201527\n",
      "\n",
      "Val RMSE: 0.421220, Val Loss: 0.177431\n",
      "\n",
      "Val RMSE: 0.406667, Val Loss: 0.164297\n",
      "\n",
      "Val RMSE: 0.408354, Val Loss: 0.165850\n",
      "\n",
      "Val RMSE: 0.403630, Val Loss: 0.160393\n",
      "\n",
      "Val RMSE: 0.403576, Val Loss: 0.161320\n",
      "\n",
      "Val RMSE: 0.404037, Val Loss: 0.161787\n",
      "\n",
      "Val RMSE: 0.404225, Val Loss: 0.160756\n",
      "\n",
      "Val RMSE: 0.402519, Val Loss: 0.159997\n",
      "\n",
      "Val RMSE: 0.402530, Val Loss: 0.160045\n",
      "\n",
      "Val RMSE: 0.402570, Val Loss: 0.159924\n",
      "\n",
      "Val RMSE: 0.402644, Val Loss: 0.160253\n",
      "\n",
      "Val RMSE: 0.403256, Val Loss: 0.160983\n",
      "\n",
      "Val RMSE: 0.403067, Val Loss: 0.160776\n",
      "\n",
      "Val RMSE: 0.403429, Val Loss: 0.161166\n",
      "\n",
      "Val RMSE: 0.402886, Val Loss: 0.160566\n",
      "\n",
      "Val RMSE: 0.403062, Val Loss: 0.160769\n",
      "\n",
      "Val RMSE: 0.402586, Val Loss: 0.160163\n",
      "\n",
      "Val RMSE: 0.402888, Val Loss: 0.160569\n",
      "\n",
      "Val RMSE: 0.402947, Val Loss: 0.160639\n",
      "\n",
      "Val RMSE: 0.403012, Val Loss: 0.160712\n",
      "\n",
      "Val RMSE: 0.402996, Val Loss: 0.160695\n",
      "\n",
      "Val RMSE: 0.402983, Val Loss: 0.160679\n",
      "\n",
      "Val RMSE: 0.402933, Val Loss: 0.160622\n",
      "\n",
      "Val RMSE: 0.402936, Val Loss: 0.160625\n",
      "\n",
      "Val RMSE: 0.402936, Val Loss: 0.160625\n",
      "\n",
      "Val RMSE: 0.438689, Val Loss: 0.193279\n",
      "\n",
      "Val RMSE: 0.407437, Val Loss: 0.162936\n",
      "\n",
      "Val RMSE: 0.402719, Val Loss: 0.159941\n",
      "\n",
      "Val RMSE: 0.404546, Val Loss: 0.160960\n",
      "\n",
      "Val RMSE: 0.402682, Val Loss: 0.160309\n",
      "\n",
      "Val RMSE: 0.402520, Val Loss: 0.160002\n",
      "\n",
      "Val RMSE: 0.402928, Val Loss: 0.160616\n",
      "\n",
      "Val RMSE: 0.402530, Val Loss: 0.160047\n",
      "\n",
      "Val RMSE: 0.403713, Val Loss: 0.161460\n",
      "\n",
      "Val RMSE: 0.403082, Val Loss: 0.160792\n",
      "\n",
      "Val RMSE: 0.403738, Val Loss: 0.161486\n",
      "\n",
      "Val RMSE: 0.402730, Val Loss: 0.160373\n",
      "\n",
      "Val RMSE: 0.403188, Val Loss: 0.160909\n",
      "\n",
      "Val RMSE: 0.402610, Val Loss: 0.160203\n",
      "\n",
      "Val RMSE: 0.402925, Val Loss: 0.160612\n",
      "\n",
      "Val RMSE: 0.402766, Val Loss: 0.160419\n",
      "\n",
      "Val RMSE: 0.402618, Val Loss: 0.160216\n",
      "\n",
      "Val RMSE: 0.402809, Val Loss: 0.160473\n",
      "\n",
      "Val RMSE: 0.402791, Val Loss: 0.160451\n",
      "\n",
      "Val RMSE: 0.402937, Val Loss: 0.160626\n",
      "\n",
      "Val RMSE: 0.402702, Val Loss: 0.160335\n",
      "\n",
      "Val RMSE: 0.402705, Val Loss: 0.160340\n",
      "\n",
      "Val RMSE: 0.402725, Val Loss: 0.160366\n",
      "\n",
      "Val RMSE: 0.402726, Val Loss: 0.160368\n",
      "\n",
      "Val RMSE: 0.402726, Val Loss: 0.160368\n",
      "\n",
      "Val RMSE: 0.421650, Val Loss: 0.173694\n",
      "\n",
      "Val RMSE: 0.415911, Val Loss: 0.169230\n",
      "\n",
      "Val RMSE: 0.410950, Val Loss: 0.165488\n",
      "\n",
      "Val RMSE: 0.402778, Val Loss: 0.159960\n",
      "\n",
      "Val RMSE: 0.402721, Val Loss: 0.159942\n",
      "\n",
      "Val RMSE: 0.402519, Val Loss: 0.159991\n",
      "\n",
      "Val RMSE: 0.402519, Val Loss: 0.159995\n",
      "\n",
      "Val RMSE: 0.402717, Val Loss: 0.159941\n",
      "\n",
      "Val RMSE: 0.402525, Val Loss: 0.159957\n",
      "\n",
      "Val RMSE: 0.402720, Val Loss: 0.159942\n",
      "\n",
      "Val RMSE: 0.402545, Val Loss: 0.159934\n",
      "\n",
      "Val RMSE: 0.402578, Val Loss: 0.160149\n",
      "\n",
      "Val RMSE: 0.402611, Val Loss: 0.159921\n",
      "\n",
      "Val RMSE: 0.402604, Val Loss: 0.159921\n",
      "\n",
      "Val RMSE: 0.402599, Val Loss: 0.159921\n",
      "\n",
      "Val RMSE: 0.402728, Val Loss: 0.159944\n",
      "\n",
      "Val RMSE: 0.402564, Val Loss: 0.160124\n",
      "\n",
      "Val RMSE: 0.402690, Val Loss: 0.159934\n",
      "\n",
      "Val RMSE: 0.402661, Val Loss: 0.159927\n",
      "\n",
      "Val RMSE: 0.402559, Val Loss: 0.159928\n",
      "\n",
      "Val RMSE: 0.402612, Val Loss: 0.159921\n",
      "\n",
      "Val RMSE: 0.402582, Val Loss: 0.159922\n",
      "\n",
      "Val RMSE: 0.402580, Val Loss: 0.159922\n",
      "\n",
      "Val RMSE: 0.402581, Val Loss: 0.159922\n",
      "\n",
      "Val RMSE: 0.402581, Val Loss: 0.159922\n",
      "\n",
      "Val RMSE: 0.411332, Val Loss: 0.165771\n",
      "\n",
      "Val RMSE: 0.423959, Val Loss: 0.175523\n",
      "\n",
      "Val RMSE: 0.406395, Val Loss: 0.162204\n",
      "\n",
      "Val RMSE: 0.406931, Val Loss: 0.162579\n",
      "\n",
      "Val RMSE: 0.404211, Val Loss: 0.160747\n",
      "\n",
      "Val RMSE: 0.403895, Val Loss: 0.160551\n",
      "\n",
      "Val RMSE: 0.403696, Val Loss: 0.160432\n",
      "\n",
      "Val RMSE: 0.404656, Val Loss: 0.161031\n",
      "\n",
      "Val RMSE: 0.402749, Val Loss: 0.159950\n",
      "\n",
      "Val RMSE: 0.402832, Val Loss: 0.159980\n",
      "\n",
      "Val RMSE: 0.402864, Val Loss: 0.159993\n",
      "\n",
      "Val RMSE: 0.403083, Val Loss: 0.160093\n",
      "\n",
      "Val RMSE: 0.403776, Val Loss: 0.160479\n",
      "\n",
      "Val RMSE: 0.403114, Val Loss: 0.160109\n",
      "\n",
      "Val RMSE: 0.403549, Val Loss: 0.160346\n",
      "\n",
      "Val RMSE: 0.403745, Val Loss: 0.160461\n",
      "\n",
      "Val RMSE: 0.403645, Val Loss: 0.160402\n",
      "\n",
      "Val RMSE: 0.403621, Val Loss: 0.160388\n",
      "\n",
      "Val RMSE: 0.403332, Val Loss: 0.160224\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(51810) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.403477, Val Loss: 0.160304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(51945) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(52393) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.403466, Val Loss: 0.160298\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(52546) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(52915) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.403522, Val Loss: 0.160330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(53029) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(53420) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.403507, Val Loss: 0.160322\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(53528) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(53937) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.403519, Val Loss: 0.160328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(54102) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(54453) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.403519, Val Loss: 0.160328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(54575) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(54803) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(54903) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(55309) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.473554, Val Loss: 0.218125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(55429) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(55888) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.428443, Val Loss: 0.179121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(56012) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(56398) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.410254, Val Loss: 0.164973\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(56501) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(56847) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.407912, Val Loss: 0.163274\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(56976) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(57360) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.408533, Val Loss: 0.163720\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(57521) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(57877) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.408355, Val Loss: 0.163591\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(58003) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(58471) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.409294, Val Loss: 0.164271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(58614) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(58985) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.409914, Val Loss: 0.164724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(59119) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(59545) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.408286, Val Loss: 0.163542\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(59661) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(60003) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.408783, Val Loss: 0.163900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(60113) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(60546) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.407085, Val Loss: 0.162687\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(60649) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(61287) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.407208, Val Loss: 0.162774\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(61412) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(62166) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.406613, Val Loss: 0.162356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(62307) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(63243) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.407202, Val Loss: 0.162770\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(63344) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(64111) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.406713, Val Loss: 0.162426\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(64241) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(65341) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.406092, Val Loss: 0.161995\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(65463) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(66351) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.406200, Val Loss: 0.162070\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(66478) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(68129) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.405882, Val Loss: 0.161852\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(68262) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(70729) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.405747, Val Loss: 0.161759\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(70875) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(73808) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.405582, Val Loss: 0.161647\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(73950) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(77423) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.405466, Val Loss: 0.161569\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(77569) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(81059) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.405373, Val Loss: 0.161506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(81233) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(84191) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.405324, Val Loss: 0.161473\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(84338) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(87602) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.405314, Val Loss: 0.161466\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(87743) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(91708) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 0.405314, Val Loss: 0.161466\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(92296) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(95221) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(95803) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lambdas = [1e-3, 3e-3, 1e-2, 2e-2, 4e-2, 6e-2, 8e-2, 1e-1]\n",
    "\n",
    "# Ridge (L2)\n",
    "ridge_train, ridge_val = [], []\n",
    "for lam in lambdas:\n",
    "    tr, va = train_conformer_with_regularization(\"ridge\", lam, n_epochs=25, lr=0.002)\n",
    "    ridge_train.append(tr)\n",
    "    ridge_val.append(va)\n",
    "\n",
    "# Lasso (L1)\n",
    "lasso_train, lasso_val = [], []\n",
    "for lam in lambdas:\n",
    "    tr, va = train_conformer_with_regularization(\"lasso\", lam, n_epochs=25, lr=0.002)\n",
    "    lasso_train.append(tr)\n",
    "    lasso_val.append(va)\n",
    "\n",
    "# Plots: two graphs, each with train and val vs lambda\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Ridge\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lambdas, ridge_train, marker='o', label='Train RMSE')\n",
    "plt.plot(lambdas, ridge_val, marker='s', label='Val RMSE')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda (L2 weight decay)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('EEGConformer Ridge: Train/Val RMSE vs Lambda')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lambdas, lasso_train, marker='o', label='Train RMSE')\n",
    "plt.plot(lambdas, lasso_val, marker='s', label='Val RMSE')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda (L1 penalty)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('EEGConformer Lasso: Train/Val RMSE vs Lambda')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Ridge:\")\n",
    "for lam, tr, va in zip(lambdas, ridge_train, ridge_val):\n",
    "    print(f\"  {lam:>8}: train={tr:.6f}, val={va:.6f}\")\n",
    "print(\"\\nLasso:\")\n",
    "for lam, tr, va in zip(lambdas, lasso_train, lasso_val):\n",
    "    print(f\"  {lam:>8}: train={tr:.6f}, val={va:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "eeg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
