{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220eb55a",
   "metadata": {},
   "source": [
    "# Challenge 1: Graph Construction for EEG Channel Relationships\n",
    "\n",
    "This notebook implements **two graph construction methods** for modeling relationships between EEG channels using Graph Neural Networks (GNN).\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Goal**: Construct meaningful graph structures that capture relationships between EEG channels for use in GNN-based models for response time prediction.\n",
    "\n",
    "**Data Structure**:\n",
    "- Input: `(batch_size, 129, 200)` - 128 EEG channels + 1 reference channel, 2 seconds @ 100Hz\n",
    "- Target: Response time in seconds (regression task)\n",
    "\n",
    "## Graph Construction Methods\n",
    "\n",
    "We'll implement two approaches:\n",
    "\n",
    "1. **Functional Connectivity Graph** (Data-driven)\n",
    "   - Computes correlation between channel time-series\n",
    "   - Captures actual signal relationships from the data\n",
    "   - Interpretable and based on real functional connections\n",
    "\n",
    "2. **Fully Connected Graph with Learnable Weights** (Model-driven)\n",
    "   - All channels connected to all others\n",
    "   - Edge weights learned during training\n",
    "   - Maximum flexibility, lets the model discover important connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c45e4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20199787",
   "metadata": {},
   "source": [
    "## 1. Load Sample Data\n",
    "\n",
    "First, we'll load a small sample of EEG data to test our graph construction methods. This will help us understand the data structure and build graphs that capture meaningful channel relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3887d19",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load data using EEGChallengeDataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meegdash\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGChallengeDataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbraindecode\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess, Preprocessor, create_windows_from_events\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meegdash\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhbn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwindows\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     annotate_trials_with_target,\n\u001b[32m      6\u001b[39m     add_aux_anchors,\n\u001b[32m      7\u001b[39m     keep_only_recordings_with,\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/eegdash/__init__.py:12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Authors: The EEGDash contributors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# License: GNU General Public License\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright the EEGDash contributors.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"EEGDash: A comprehensive platform for EEG data management and analysis.\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[33;03mEEGDash provides a unified interface for accessing, querying, and analyzing large-scale\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mEEG datasets. It integrates with cloud storage, MongoDB databases, and machine learning\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mframeworks to streamline EEG research workflows.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGDash, EEGDashDataset\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGChallengeDataset\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhbn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/eegdash/api.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Mapping\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmne\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdocstring_inheritance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumpyDocstringInheritanceInitMeta\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmne\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _soft_import\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/mne/__init__.py:35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# initialize logging\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_log_level, set_log_file\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mset_log_level\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m set_log_file()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/mne/utils/_logging.py:210\u001b[39m, in \u001b[36mset_log_level\u001b[39m\u001b[34m(verbose, return_old_level, add_frames)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Set the logging level.\u001b[39;00m\n\u001b[32m    190\u001b[39m \n\u001b[32m    191\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m \u001b[33;03m    The old level. Only returned if ``return_old_level`` is True.\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m old_verbose = logger.level\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m verbose = \u001b[43m_parse_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose != old_verbose:\n\u001b[32m    213\u001b[39m     logger.setLevel(verbose)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/mne/utils/_logging.py:225\u001b[39m, in \u001b[36m_parse_verbose\u001b[39m\u001b[34m(verbose)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_parse_verbose\u001b[39m(verbose):\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheck\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _check_option, _validate_type\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m    228\u001b[39m     _validate_type(verbose, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[33m\"\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/mne/utils/check.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdefaults\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HEAD_SIZE_DEFAULT, _handle_default\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _compare_version, _median_complex\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _record_warnings, _verbose_safe_false, logger, verbose, warn\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ensure_int\u001b[39m(x, name=\u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m, must_be=\u001b[33m\"\u001b[39m\u001b[33man int\u001b[39m\u001b[33m\"\u001b[39m, *, extra=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/mne/fixes.py:605\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# Numba (optional requirement)\u001b[39;00m\n\u001b[32m    602\u001b[39m \n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# Here we choose different defaults to speed things up by default\u001b[39;00m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _compare_version(numba.__version__, \u001b[33m\"\u001b[39m\u001b[33m<\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0.56.4\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    608\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/numba/__init__.py:92\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Re-export decorators\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (cfunc, jit, njit, stencil,\n\u001b[32m     93\u001b[39m                                    jit_module)\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Re-export vectorize decorators and the thread layer querying function\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mufunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (vectorize, guvectorize, threading_layer,\n\u001b[32m     97\u001b[39m                             get_num_threads, set_num_threads,\n\u001b[32m     98\u001b[39m                             set_parallel_chunksize, get_parallel_chunksize,\n\u001b[32m     99\u001b[39m                             get_thread_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/numba/core/decorators.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MappingProxyType\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeprecationError, NumbaDeprecationWarning\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstencils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstencil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stencil\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, extending, sigutils, registry\n\u001b[32m     16\u001b[39m _logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/numba/stencils/stencil.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllvmlite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ir \u001b[38;5;28;01mas\u001b[39;00m lir\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types, typing, utils, ir, config, ir_utils, registry\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtemplates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (CallableTemplate, signature,\n\u001b[32m     13\u001b[39m                                          infer_global, AbstractTemplate)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lower_builtin\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/numba/core/registry.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m threadsafe_cached_property \u001b[38;5;28;01mas\u001b[39;00m cached_property\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdescriptors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TargetDescriptor\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, typing, dispatcher, cpu\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Default CPU target descriptors\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCPUTarget\u001b[39;00m(TargetDescriptor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/numba/core/dispatcher.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypeof\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Purpose, typeof\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbytecode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_code_object\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcaching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NullCache, FunctionCache\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m entrypoints\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mev\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/ese 538/eeg_challenge/.venv/lib/python3.11/site-packages/numba/core/caching.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muuid\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mappdirs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AppDirs\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mzipfile\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1178\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1149\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:936\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1032\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1131\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load data using EEGChallengeDataset\n",
    "from eegdash.dataset import EEGChallengeDataset\n",
    "from braindecode.preprocessing import preprocess, Preprocessor, create_windows_from_events\n",
    "from eegdash.hbn.windows import (\n",
    "    annotate_trials_with_target,\n",
    "    add_aux_anchors,\n",
    "    keep_only_recordings_with,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "RELEASE_ID = 1  # Using release 1 from merged data\n",
    "RELEASE_DIR = Path(\"data_merged\") / f\"release_{RELEASE_ID}\"\n",
    "EPOCH_LEN_S = 2.0\n",
    "SFREQ = 100\n",
    "\n",
    "print(f\"ðŸ“ Loading data from: {RELEASE_DIR}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_ccd = EEGChallengeDataset(\n",
    "    task=\"contrastChangeDetection\",\n",
    "    release=f\"R{RELEASE_ID}\",\n",
    "    cache_dir=RELEASE_DIR,\n",
    "    mini=False,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Loaded {len(dataset_ccd.datasets)} recordings\")\n",
    "\n",
    "# Preprocess\n",
    "transformation_offline = [\n",
    "    Preprocessor(\n",
    "        annotate_trials_with_target,\n",
    "        target_field=\"rt_from_stimulus\",\n",
    "        epoch_length=EPOCH_LEN_S,\n",
    "        require_stimulus=True,\n",
    "        require_response=True,\n",
    "        apply_on_array=False,\n",
    "    ),\n",
    "    Preprocessor(add_aux_anchors, apply_on_array=False),\n",
    "]\n",
    "preprocess(dataset_ccd, transformation_offline, n_jobs=1)\n",
    "\n",
    "# Create windows\n",
    "ANCHOR = \"stimulus_anchor\"\n",
    "SHIFT_AFTER_STIM = 0.5\n",
    "WINDOW_LEN = 2.0\n",
    "\n",
    "dataset = keep_only_recordings_with(ANCHOR, dataset_ccd)\n",
    "single_windows = create_windows_from_events(\n",
    "    dataset,\n",
    "    mapping={ANCHOR: 0},\n",
    "    trial_start_offset_samples=int(SHIFT_AFTER_STIM * SFREQ),\n",
    "    trial_stop_offset_samples=int((SHIFT_AFTER_STIM + WINDOW_LEN) * SFREQ),\n",
    "    window_size_samples=int(EPOCH_LEN_S * SFREQ),\n",
    "    window_stride_samples=SFREQ,\n",
    "    preload=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Created {len(single_windows)} windows\")\n",
    "\n",
    "# Extract sample data for graph construction\n",
    "sample_data = []\n",
    "sample_targets = []\n",
    "for i in range(min(100, len(single_windows))):  # Use first 100 windows\n",
    "    window = single_windows[i]\n",
    "    X, y = window[0], window[1]\n",
    "    if isinstance(X, torch.Tensor):\n",
    "        X = X.numpy()\n",
    "    sample_data.append(X)\n",
    "    sample_targets.append(y)\n",
    "\n",
    "sample_data = np.array(sample_data)  # (100, 129, 200)\n",
    "sample_targets = np.array(sample_targets)  # (100,)\n",
    "\n",
    "print(f\"\\nðŸ“Š Sample data shape: {sample_data.shape}\")\n",
    "print(f\"   Channels: {sample_data.shape[1]} (128 EEG + 1 reference)\")\n",
    "print(f\"   Time points: {sample_data.shape[2]} (2 seconds @ 100Hz)\")\n",
    "print(f\"   Target range: {sample_targets.min():.2f} - {sample_targets.max():.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6fab6",
   "metadata": {},
   "source": [
    "## 2. Graph Construction Methods\n",
    "\n",
    "We'll implement two graph construction approaches that are well-suited for EEG data where we don't have spatial channel information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d21c20",
   "metadata": {},
   "source": [
    "### Method 1: Functional Connectivity Graph\n",
    "\n",
    "**Concept**: Channels that have correlated signals are likely functionally related. We compute pairwise correlations between all channel time-series and create edges for channels with strong correlations.\n",
    "\n",
    "**Advantages**:\n",
    "- **Data-driven**: Based on actual signal relationships in your data\n",
    "- **Interpretable**: You can see which channels are functionally connected\n",
    "- **Sparse**: Only connects channels with meaningful relationships (reduces noise)\n",
    "\n",
    "**Steps**:\n",
    "1. Extract time-series for each of the 128 EEG channels\n",
    "2. Compute Pearson correlation matrix across all channel pairs\n",
    "3. Threshold correlations (e.g., |correlation| > 0.3) to create edges\n",
    "4. Use correlation values as edge weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a70e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_functional_connectivity_graph(eeg_data, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Build graph from functional connectivity (correlation) between channels.\n",
    "    \n",
    "    This function computes pairwise correlations between all EEG channels and creates\n",
    "    edges for channel pairs with correlation above the threshold.\n",
    "    \n",
    "    Args:\n",
    "        eeg_data: (batch, 129, 200) - EEG data (uses first 128 channels, excludes reference)\n",
    "        threshold: Minimum absolute correlation to create an edge (default: 0.3)\n",
    "    \n",
    "    Returns:\n",
    "        edge_index: (2, num_edges) - Graph edges as source-target pairs\n",
    "        edge_weights: (num_edges,) - Edge weights (correlation values)\n",
    "        corr_matrix: (128, 128) - Full correlation matrix for visualization\n",
    "    \"\"\"\n",
    "    # Use only EEG channels (exclude reference channel at index 128)\n",
    "    eeg_channels = eeg_data[:, :128, :]  # (batch, 128, 200)\n",
    "    \n",
    "    # Reshape to (batch*200, 128) - concatenate all time points across all samples\n",
    "    # This gives us a long time-series for each channel to compute correlations\n",
    "    signals = eeg_channels.transpose(0, 2, 1).reshape(-1, 128)\n",
    "    \n",
    "    # Compute Pearson correlation matrix\n",
    "    corr_matrix = np.corrcoef(signals.T)  # (128, 128)\n",
    "    \n",
    "    # Create adjacency matrix: connect channels with |correlation| > threshold\n",
    "    adj_matrix = np.abs(corr_matrix) > threshold\n",
    "    np.fill_diagonal(adj_matrix, False)  # Remove self-loops\n",
    "    \n",
    "    # Convert adjacency matrix to edge list format (PyTorch Geometric style)\n",
    "    edge_index = np.array(np.where(adj_matrix))  # (2, num_edges)\n",
    "    \n",
    "    # Edge weights = correlation values (can be positive or negative)\n",
    "    edge_weights = corr_matrix[adj_matrix]\n",
    "    \n",
    "    # Print statistics\n",
    "    num_edges = edge_index.shape[1]\n",
    "    num_possible = 128 * 127 // 2  # Maximum edges in undirected graph\n",
    "    \n",
    "    print(f\"ðŸ“Š Functional Connectivity Graph:\")\n",
    "    print(f\"   Correlation threshold: {threshold}\")\n",
    "    print(f\"   Number of edges: {num_edges:,} / {num_possible:,} possible\")\n",
    "    print(f\"   Edge density: {num_edges / num_possible:.2%}\")\n",
    "    print(f\"   Mean |correlation|: {np.abs(edge_weights).mean():.3f}\")\n",
    "    print(f\"   Max |correlation|: {np.abs(edge_weights).max():.3f}\")\n",
    "    \n",
    "    return edge_index, edge_weights, corr_matrix\n",
    "\n",
    "# Build functional connectivity graph\n",
    "print(\"Building functional connectivity graph...\")\n",
    "fc_edge_index, fc_edge_weights, fc_corr_matrix = build_functional_connectivity_graph(\n",
    "    sample_data, \n",
    "    threshold=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Graph created!\")\n",
    "print(f\"   Edge index shape: {fc_edge_index.shape}\")\n",
    "print(f\"   Edge weights shape: {fc_edge_weights.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90baff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize functional connectivity graph\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Full correlation matrix (shows all correlation values)\n",
    "im1 = axes[0].imshow(fc_corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0].set_title('Correlation Matrix\\n(All channel pairs)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Channel Index')\n",
    "axes[0].set_ylabel('Channel Index')\n",
    "plt.colorbar(im1, ax=axes[0], label='Correlation')\n",
    "\n",
    "# Right: Binary adjacency matrix (shows which channels are connected)\n",
    "adj_matrix = np.abs(fc_corr_matrix) > 0.3\n",
    "np.fill_diagonal(adj_matrix, False)\n",
    "im2 = axes[1].imshow(adj_matrix.astype(int), cmap='Blues', vmin=0, vmax=1)\n",
    "axes[1].set_title('Graph Adjacency Matrix\\n(Connected channels)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Channel Index')\n",
    "axes[1].set_ylabel('Channel Index')\n",
    "plt.colorbar(im2, ax=axes[1], label='Connected (1) / Not Connected (0)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“ˆ Visualization:\")\n",
    "print(\"   Left: Correlation values between all channel pairs (red=negative, blue=positive)\")\n",
    "print(\"   Right: Binary adjacency showing which channels are connected (threshold=0.3)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32450940",
   "metadata": {},
   "source": [
    "### Method 2: Fully Connected Graph with Learnable Weights\n",
    "\n",
    "**Concept**: Connect all channels to all other channels, and let the model learn which connections are important during training. This is typically implemented using Graph Attention Networks (GAT) or learnable adjacency matrices.\n",
    "\n",
    "**Advantages**:\n",
    "- **Maximum flexibility**: Model can discover any channel relationships\n",
    "- **No assumptions**: Doesn't require thresholding or prior knowledge\n",
    "- **Adaptive**: Learns task-specific connections\n",
    "\n",
    "**Trade-offs**:\n",
    "- **More parameters**: Fully connected means more edges to learn\n",
    "- **Less interpretable**: Learned weights may not have clear meaning\n",
    "- **Computationally expensive**: More edges = more computation\n",
    "\n",
    "**Implementation**: We'll create a fully connected graph structure. The actual learning of edge weights happens during model training (e.g., using GAT attention mechanisms).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fully_connected_graph(n_channels=128, init_method='small_random'):\n",
    "    \"\"\"\n",
    "    Build fully connected graph (all channels connected to all others).\n",
    "    \n",
    "    This creates the graph structure. The actual edge weights will be learned\n",
    "    during training using mechanisms like Graph Attention Networks (GAT).\n",
    "    \n",
    "    Args:\n",
    "        n_channels: Number of channels (128)\n",
    "        init_method: How to initialize edge weights\n",
    "            - 'small_random': Small random values (good for learnable weights)\n",
    "            - 'uniform': Uniform weights (normalized)\n",
    "            - 'ones': All weights = 1.0\n",
    "    \n",
    "    Returns:\n",
    "        edge_index: (2, num_edges) - Graph edges (fully connected, no self-loops)\n",
    "        edge_weights: (num_edges,) - Initial edge weights (will be learned)\n",
    "        adj_matrix: (n_channels, n_channels) - Adjacency matrix for visualization\n",
    "    \"\"\"\n",
    "    # Create fully connected adjacency (all channels connected, except self-loops)\n",
    "    adj_matrix = np.ones((n_channels, n_channels), dtype=bool)\n",
    "    np.fill_diagonal(adj_matrix, False)  # Remove self-loops\n",
    "    \n",
    "    # Convert to edge list format\n",
    "    edge_index = np.array(np.where(adj_matrix))  # (2, num_edges)\n",
    "    \n",
    "    # Initialize edge weights (these will be learned/updated during training)\n",
    "    num_edges = edge_index.shape[1]\n",
    "    \n",
    "    if init_method == 'small_random':\n",
    "        # Small random values - good starting point for learnable weights\n",
    "        edge_weights = np.random.randn(num_edges) * 0.01\n",
    "    elif init_method == 'uniform':\n",
    "        # Uniform weights (normalized)\n",
    "        edge_weights = np.ones(num_edges) / num_edges\n",
    "    elif init_method == 'ones':\n",
    "        # All weights = 1.0\n",
    "        edge_weights = np.ones(num_edges)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown init_method: {init_method}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    num_possible = n_channels * (n_channels - 1) // 2\n",
    "    \n",
    "    print(f\"ðŸ“Š Fully Connected Graph:\")\n",
    "    print(f\"   Initialization: {init_method}\")\n",
    "    print(f\"   Number of edges: {num_edges:,} / {num_possible:,} possible\")\n",
    "    print(f\"   Edge density: {num_edges / num_possible:.2%} (fully connected)\")\n",
    "    print(f\"   Mean edge weight: {edge_weights.mean():.6f}\")\n",
    "    print(f\"   Std edge weight: {edge_weights.std():.6f}\")\n",
    "    print(f\"\\n   Note: Edge weights will be learned during model training\")\n",
    "    print(f\"   (e.g., using Graph Attention Network attention mechanisms)\")\n",
    "    \n",
    "    return edge_index, edge_weights, adj_matrix\n",
    "\n",
    "# Build fully connected graph\n",
    "print(\"Building fully connected graph...\")\n",
    "fc_full_edge_index, fc_full_edge_weights, fc_full_adj_matrix = build_fully_connected_graph(\n",
    "    n_channels=128,\n",
    "    init_method='small_random'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Graph created!\")\n",
    "print(f\"   Edge index shape: {fc_full_edge_index.shape}\")\n",
    "print(f\"   Edge weights shape: {fc_full_edge_weights.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944dd1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fully connected graph\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Binary adjacency matrix (should be all ones except diagonal)\n",
    "im1 = axes[0].imshow(fc_full_adj_matrix.astype(int), cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_title('Fully Connected Adjacency Matrix\\n(All channels connected)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Channel Index')\n",
    "axes[0].set_ylabel('Channel Index')\n",
    "plt.colorbar(im1, ax=axes[0], label='Connected (1) / Not Connected (0)')\n",
    "\n",
    "# Right: Initial edge weights (reshaped to matrix for visualization)\n",
    "weight_matrix = np.zeros((128, 128))\n",
    "weight_matrix[fc_full_edge_index[0], fc_full_edge_index[1]] = fc_full_edge_weights\n",
    "im2 = axes[1].imshow(weight_matrix, cmap='viridis')\n",
    "axes[1].set_title('Initial Edge Weights\\n(Will be learned during training)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Channel Index')\n",
    "axes[1].set_ylabel('Channel Index')\n",
    "plt.colorbar(im2, ax=axes[1], label='Initial Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“ˆ Visualization:\")\n",
    "print(\"   Left: Binary adjacency showing all channels are connected (except self-loops)\")\n",
    "print(\"   Right: Initial edge weights (small random values that will be learned)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c03a7",
   "metadata": {},
   "source": [
    "## 3. Comparison of Graph Construction Methods\n",
    "\n",
    "Let's compare the two methods to understand their characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two graph construction methods\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Method': [\n",
    "        'Functional Connectivity',\n",
    "        'Fully Connected (Learnable)'\n",
    "    ],\n",
    "    'Number of Edges': [\n",
    "        fc_edge_index.shape[1],\n",
    "        fc_full_edge_index.shape[1]\n",
    "    ],\n",
    "    'Edge Density': [\n",
    "        f\"{fc_edge_index.shape[1] / (128 * 127 // 2):.2%}\",\n",
    "        f\"{fc_full_edge_index.shape[1] / (128 * 127 // 2):.2%}\"\n",
    "    ],\n",
    "    'Edge Weight Source': [\n",
    "        'Correlation values (from data)',\n",
    "        'Learnable (initialized, then trained)'\n",
    "    ],\n",
    "    'Interpretability': [\n",
    "        'High - shows actual signal relationships',\n",
    "        'Low - learned weights may not be interpretable'\n",
    "    ],\n",
    "    'Flexibility': [\n",
    "        'Medium - fixed by data correlations',\n",
    "        'High - model learns optimal connections'\n",
    "    ],\n",
    "    'Computational Cost': [\n",
    "        'Lower - sparse graph',\n",
    "        'Higher - fully connected'\n",
    "    ],\n",
    "    'Best For': [\n",
    "        'Understanding channel relationships, interpretable models',\n",
    "        'Maximum model flexibility, task-specific learning'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"ðŸ“Š Graph Construction Methods Comparison:\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*90)\n",
    "print(\"1. Functional Connectivity: Data-driven, interpretable, sparse\")\n",
    "print(\"   â†’ Use when you want to understand channel relationships\")\n",
    "print(\"   â†’ Good starting point for exploration\")\n",
    "print()\n",
    "print(\"2. Fully Connected (Learnable): Maximum flexibility, task-adaptive\")\n",
    "print(\"   â†’ Use when you want the model to discover optimal connections\")\n",
    "print(\"   â†’ Requires more parameters and computation\")\n",
    "print()\n",
    "print(\"ðŸ’¡ Recommendation: Try both and compare performance!\")\n",
    "print(\"   Start with functional connectivity for interpretability,\")\n",
    "print(\"   then try fully connected if you need better performance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Convert to PyTorch Format\n",
    "\n",
    "For use with PyTorch Geometric or other GNN libraries, we need to convert the graphs to PyTorch tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e14ae",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pytorch(edge_index, edge_weights=None):\n",
    "    \"\"\"\n",
    "    Convert numpy arrays to PyTorch tensors for use with PyTorch Geometric.\n",
    "    \n",
    "    Args:\n",
    "        edge_index: (2, num_edges) numpy array - source and target node indices\n",
    "        edge_weights: (num_edges,) numpy array (optional) - edge weights\n",
    "    \n",
    "    Returns:\n",
    "        edge_index_tensor: torch.LongTensor - edge connections\n",
    "        edge_weights_tensor: torch.FloatTensor (if provided) - edge weights\n",
    "    \"\"\"\n",
    "    edge_index_tensor = torch.from_numpy(edge_index).long()\n",
    "    \n",
    "    if edge_weights is not None:\n",
    "        edge_weights_tensor = torch.from_numpy(edge_weights).float()\n",
    "        return edge_index_tensor, edge_weights_tensor\n",
    "    else:\n",
    "        return edge_index_tensor\n",
    "\n",
    "# Convert both graphs to PyTorch tensors\n",
    "print(\"Converting graphs to PyTorch format...\")\n",
    "fc_edge_index_torch, fc_edge_weights_torch = convert_to_pytorch(fc_edge_index, fc_edge_weights)\n",
    "fc_full_edge_index_torch, fc_full_edge_weights_torch = convert_to_pytorch(\n",
    "    fc_full_edge_index, fc_full_edge_weights\n",
    ")\n",
    "\n",
    "print(\"âœ… Graphs converted to PyTorch tensors:\")\n",
    "print(f\"   Functional Connectivity:\")\n",
    "print(f\"      Edge index: {fc_edge_index_torch.shape}\")\n",
    "print(f\"      Edge weights: {fc_edge_weights_torch.shape}\")\n",
    "print(f\"   Fully Connected:\")\n",
    "print(f\"      Edge index: {fc_full_edge_index_torch.shape}\")\n",
    "print(f\"      Edge weights: {fc_full_edge_weights_torch.shape}\")\n",
    "\n",
    "# Store graphs in a dictionary for easy access\n",
    "graphs = {\n",
    "    'functional': {\n",
    "        'edge_index': fc_edge_index_torch,\n",
    "        'edge_weights': fc_edge_weights_torch,\n",
    "        'description': 'Functional connectivity (correlation-based, sparse)'\n",
    "    },\n",
    "    'fully_connected': {\n",
    "        'edge_index': fc_full_edge_index_torch,\n",
    "        'edge_weights': fc_full_edge_weights_torch,\n",
    "        'description': 'Fully connected (learnable weights, dense)'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… Graphs stored in dictionary 'graphs' for use in model\")\n",
    "print(f\"   Access with: graphs['functional'] or graphs['fully_connected']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de8833",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "Now that we have two graph construction methods ready, you can use them in your GNN model:\n",
    "\n",
    "### Using the Graphs\n",
    "\n",
    "Both graphs are stored in the `graphs` dictionary with the following structure:\n",
    "- `graphs['functional']` - Functional connectivity graph (sparse, data-driven)\n",
    "- `graphs['fully_connected']` - Fully connected graph (dense, learnable)\n",
    "\n",
    "Each graph contains:\n",
    "- `edge_index`: `(2, num_edges)` tensor - edge connections (source â†’ target)\n",
    "- `edge_weights`: `(num_edges,)` tensor - edge weights\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "# Get functional connectivity graph\n",
    "func_graph = graphs['functional']\n",
    "edge_index = func_graph['edge_index']  # (2, num_edges)\n",
    "edge_weights = func_graph['edge_weights']  # (num_edges,)\n",
    "\n",
    "# Use with PyTorch Geometric\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "\n",
    "# Example: Graph Convolutional Network\n",
    "class EEGGNN(nn.Module):\n",
    "    def __init__(self, num_channels=128, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_channels, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        # ... rest of model\n",
    "```\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Start with functional connectivity**: It's interpretable and based on real data relationships\n",
    "2. **Try fully connected if needed**: If functional connectivity doesn't perform well, try the learnable approach\n",
    "3. **Combine with temporal models**: Use GNN for spatial relationships + GRU/LSTM for temporal patterns\n",
    "4. **Experiment with thresholds**: For functional connectivity, try different correlation thresholds (0.2, 0.3, 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe165e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afaf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
